{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guessing gender\n",
    "# Collect 1500 tweets matching words related to Blockchain\n",
    "import configparser\n",
    "import sys\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from TwitterAPI import TwitterAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Read census_names and tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4014 female and 1146 male names\n"
     ]
    }
   ],
   "source": [
    "def read_census_names():\n",
    "    \"\"\"\n",
    "    Read census names collected in the collect python script.\n",
    "\n",
    "    Returns:\n",
    "        Two lists of male_names and female_names\n",
    "    \"\"\"\n",
    "    male_names = pickle.load(open('../data/collect/male_names.pkl', 'rb'))\n",
    "    female_names = pickle.load(open('../data/collect/female_names.pkl', 'rb'))\n",
    "    return male_names, female_names\n",
    "\n",
    "# 0 - Establish twitter connection and read all the names picked from the U.S. census.\n",
    "male_names, female_names = read_census_names()\n",
    "print('found %d female and %d male names' % (len(female_names), len(male_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Twitter connection.\n"
     ]
    }
   ],
   "source": [
    "def get_twitter(config_file):\n",
    "    \"\"\" Read the config_file and construct an instance of TwitterAPI.\n",
    "    Args:\n",
    "      config_file ... A config file in ConfigParser format with Twitter credentials\n",
    "    Returns:\n",
    "      An instance of TwitterAPI.\n",
    "    \"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "twitter = get_twitter('../twitter.cfg')\n",
    "print('Established Twitter connection.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_real_time_tweets(filename):\n",
    "    \"\"\"Read real time tweets retrieved during collect phase\n",
    "\n",
    "    Params:\n",
    "        filename.....The file where the tweets are stored.\n",
    "    Returns:\n",
    "        The list of real time tweets\n",
    "    \"\"\"\n",
    "    return pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_name(tweet):\n",
    "    \"\"\"\n",
    "    Get the first name from a twitter object.\n",
    "    \n",
    "    Params:\n",
    "        tweet....The Twitter object from where to pick the user name.\n",
    "    Returns:\n",
    "        The user first name in lower letters.\n",
    "    \"\"\"\n",
    "    if 'user' in tweet and 'name' in tweet['user']:\n",
    "        parts = tweet['user']['name'].split()\n",
    "        if len(parts) > 0:\n",
    "            return parts[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/collect/real-time-tweets.pkl'\n",
    "tweets = read_real_time_tweets(filename)\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled 5000 tweets\n",
      "top names: [('john', 74), ('michael', 60), ('chris', 52), ('mike', 47), ('kevin', 47), ('james', 46), ('ryan', 42), ('jeff', 41), ('david', 38), ('brian', 36)]\n"
     ]
    }
   ],
   "source": [
    "print('sampled %d tweets' % len(tweets))\n",
    "print('top names:', Counter(get_first_name(t) for t in tweets).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test tweet:\n",
      "\tscreen_name=MekaPye100\n",
      "\tname=Tomeka  Dorsey\n",
      "\tdescr=I AM BY ALL MEANS A TENACIOUS INDIVIDUAL!\n",
      "\ttext=@JTthepodcaster It is, my mother made me watch it 1001 times...if they ever remake it you should try out for the part\n",
      "top languages: [('en', 5000)]\n"
     ]
    }
   ],
   "source": [
    "test_tweet = tweets[1]\n",
    "print('test tweet:\\n\\tscreen_name=%s\\n\\tname=%s\\n\\tdescr=%s\\n\\ttext=%s' %\n",
    "      (test_tweet['user']['screen_name'],\n",
    "       test_tweet['user']['name'],\n",
    "       test_tweet['user']['description'],\n",
    "       test_tweet['text']))\n",
    "print('top languages:', Counter(t['lang'] for t in tweets).most_common(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tokenize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(string, lowercase, keep_punctuation, prefix, collapse_urls, collapse_mentions):\n",
    "    \"\"\" \n",
    "    Split a string into tokens.\n",
    "    If keep_internal_punct is False, then return only the alphanumerics (letters, numbers and underscore).\n",
    "    If keep_internal_punct is True, then also retain punctuation that\n",
    "    is inside of a word. E.g., in the example below, the token \"isn't\"\n",
    "    is maintained when keep_internal_punct=True; otherwise, it is\n",
    "    split into \"isn\" and \"t\" tokens\n",
    "    \n",
    "    Params:\n",
    "        string................The string that needs to be tokenized.\n",
    "        lowercase.............Boolean indicating if we want the text to be convert to lowercase.\n",
    "        keep_punctuation......Boolean indicating if we want to keep punctuation\n",
    "        prefix................Prefix to add to each obtained token. (will use for identifying what part is being tokenized, e.g. prefix d= for description)\n",
    "        collapse_urls.........Boolean indicating if we ant to collapse the urls in the text. (e.g. @something)\n",
    "        collapse_meentions....Boolean indicating if we ant to collapse the mmentions in the text. (e.g. #smth)\n",
    "    Returns:\n",
    "        An array containing the tokenized string.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return []\n",
    "    if lowercase:\n",
    "        string = string.lower()\n",
    "    tokens = []\n",
    "    if collapse_urls:\n",
    "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
    "    if collapse_mentions:\n",
    "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
    "    if keep_punctuation:\n",
    "        tokens = string.split()\n",
    "    else:\n",
    "        tokens = re.sub('\\W+', ' ', string).split()\n",
    "    if prefix:\n",
    "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d=i',\n",
       " 'd=am',\n",
       " 'd=by',\n",
       " 'd=all',\n",
       " 'd=means',\n",
       " 'd=a',\n",
       " 'd=tenacious',\n",
       " 'd=individual']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(test_tweet['user']['description'], lowercase=True,\n",
    "         keep_punctuation=False, prefix='d=',\n",
    "         collapse_urls=True, collapse_mentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet2tokens(tweet, use_descr=True, lowercase=True, keep_punctuation=True, descr_prefix='d=', collapse_urls=True, collapse_mentions=True):\n",
    "    \"\"\"\n",
    "    Convert a tweet into a list of tokens, from the tweet text and optionally the\n",
    "    user description.\n",
    "    \n",
    "    Params:\n",
    "        tweet.................The tweet that needs to be tokenized.\n",
    "        user_descr............Boolean to indicate if we want to tokenize the user description too.\n",
    "        lowercase.............Boolean indicating if we want the text to be convert to lowercase.\n",
    "        keep_punctuation......Boolean indicating if we want to keep punctuation\n",
    "        descr_prefix..........Prefix to add to the tokenization of the description.\n",
    "        collapse_urls.........Boolean indicating if we ant to collapse the urls in the text. (e.g. @something)\n",
    "        collapse_meentions....Boolean indicating if we ant to collapse the mmentions in the text. (e.g. #smth)\n",
    "    \"\"\"\n",
    "    # When tokenizing the text, do not add any prefix.\n",
    "    tokens = tokenize(tweet['text'], lowercase, keep_punctuation, None, collapse_urls, collapse_mentions)\n",
    "    if use_descr:\n",
    "        tokens.extend(tokenize(tweet['user']['description'], lowercase,\n",
    "                               keep_punctuation, descr_prefix,\n",
    "                               collapse_urls, collapse_mentions))\n",
    "    return tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
