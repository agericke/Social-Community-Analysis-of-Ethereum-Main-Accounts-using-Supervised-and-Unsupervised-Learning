{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classify data.\n",
    "\"\"\"\n",
    "# Guessing gender\n",
    "# Collect 1500 tweets matching words related to Blockchain\n",
    "import configparser\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "from TwitterAPI import TwitterAPI\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def read_census_names():\n",
    "\t\"\"\"\n",
    "\tRead census names collected in the collect python script.\n",
    "\n",
    "\tReturns:\n",
    "\t\tTwo lists of male_names and female_names\n",
    "\t\"\"\"\n",
    "\tmale_names = pickle.load(open('../data/collect/male_names.pkl', 'rb'))\n",
    "\tfemale_names = pickle.load(open('../data/collect/female_names.pkl', 'rb'))\n",
    "\treturn male_names, female_names\n",
    "\n",
    "\n",
    "def read_real_time_tweets(filename):\n",
    "\t\"\"\"Read real time tweets retrieved during collect phase\n",
    "\t\n",
    "\tParams:\n",
    "\t\tfilename.....The file where the tweets are stored.\n",
    "\tReturns:\n",
    "\t\tThe list of real time tweets\n",
    "\t\"\"\"\n",
    "\treturn pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "def get_first_name(tweet):\n",
    "    \"\"\"\n",
    "    Get the first name from a twitter object.\n",
    "    \n",
    "    Params:\n",
    "        tweet....The Twitter object from where to pick the user name.\n",
    "    Returns:\n",
    "        The user first name in lower letters.\n",
    "    \"\"\"\n",
    "    if 'user' in tweet and 'name' in tweet['user']:\n",
    "        parts = tweet['user']['name'].split()\n",
    "        if len(parts) > 0:\n",
    "            return parts[0].lower()\n",
    "\n",
    "\n",
    "def tokenize(string, lowercase, keep_punctuation, prefix, collapse_urls, collapse_mentions):\n",
    "    \"\"\" \n",
    "    Split a string into tokens.\n",
    "    If keep_internal_punct is False, then return only the alphanumerics (letters, numbers and underscore).\n",
    "    If keep_internal_punct is True, then also retain punctuation that\n",
    "    is inside of a word. E.g., in the example below, the token \"isn't\"\n",
    "    is maintained when keep_internal_punct=True; otherwise, it is\n",
    "    split into \"isn\" and \"t\" tokens\n",
    "    \n",
    "    Params:\n",
    "        string................The string that needs to be tokenized.\n",
    "        lowercase.............Boolean indicating if we want the text to be convert to lowercase.\n",
    "        keep_punctuation......Boolean indicating if we want to keep punctuation\n",
    "        prefix................Prefix to add to each obtained token. (will use for identifying what part is being tokenized, e.g. prefix d= for description)\n",
    "        collapse_urls.........Boolean indicating if we ant to collapse the urls in the text. (e.g. @something)\n",
    "        collapse_meentions....Boolean indicating if we ant to collapse the mmentions in the text. (e.g. #smth)\n",
    "    Returns:\n",
    "        An array containing the tokenized string.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return []\n",
    "    if lowercase:\n",
    "        string = string.lower()\n",
    "    tokens = []\n",
    "    if collapse_urls:\n",
    "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
    "    if collapse_mentions:\n",
    "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
    "    if keep_punctuation:\n",
    "        tokens = string.split()\n",
    "    else:\n",
    "        tokens = re.sub('\\W+', ' ', string).split()\n",
    "    if prefix:\n",
    "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tweet2tokens(tweet, use_descr=True, lowercase=True, keep_punctuation=True, descr_prefix='d=', collapse_urls=True, collapse_mentions=True):\n",
    "    \"\"\"\n",
    "    Convert a tweet into a list of tokens, from the tweet text and optionally the\n",
    "    user description.\n",
    "    \n",
    "    Params:\n",
    "        tweet.................The tweet that needs to be tokenized.\n",
    "        user_descr............Boolean to indicate if we want to tokenize the user description too.\n",
    "        lowercase.............Boolean indicating if we want the text to be convert to lowercase.\n",
    "        keep_punctuation......Boolean indicating if we want to keep punctuation\n",
    "        descr_prefix..........Prefix to add to the tokenization of the description.\n",
    "        collapse_urls.........Boolean indicating if we ant to collapse the urls in the text. (e.g. @something)\n",
    "        collapse_meentions....Boolean indicating if we ant to collapse the mmentions in the text. (e.g. #smth)\n",
    "    \"\"\"\n",
    "    # When tokenizing the text, do not add any prefix.\n",
    "    tokens = tokenize(tweet['text'], lowercase, keep_punctuation, None, collapse_urls, collapse_mentions)\n",
    "    if use_descr:\n",
    "        tokens.extend(tokenize(tweet['user']['description'], lowercase, keep_punctuation, descr_prefix,\n",
    "                               collapse_urls, collapse_mentions))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def generate_all_opts():\n",
    "    \"\"\"\n",
    "    Enumerate all possible classifier settings and compute.\n",
    "    \"\"\"\n",
    "    use_descr_opts = [True, False]\n",
    "    lowercase_opts = [True, False]\n",
    "    keep_punctuation_opts = [True, False]\n",
    "    descr_prefix_opts = ['d=', '']\n",
    "    url_opts = [True, False]\n",
    "    mention_opts = [True, False]\n",
    "    argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention']\n",
    "    option_iter = product(use_descr_opts, lowercase_opts, keep_punctuation_opts,\n",
    "                       descr_prefix_opts, url_opts, mention_opts)\n",
    "    \n",
    "    return argnames, option_iter\n",
    "\n",
    "\n",
    "def make_vocabulary(tokens_list):\n",
    "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
    "    for tokens in tokens_list:\n",
    "        for token in tokens:\n",
    "            vocabulary[token]\n",
    "    return vocabulary\n",
    "\n",
    " \n",
    "def make_feature_matrix(tweets, tokens_list, vocabulary, test=False):\n",
    "    X = lil_matrix((len(tweets), len(vocabulary)))\n",
    "    for i, tokens in enumerate(tokens_list):\n",
    "        for token in tokens:\n",
    "            if token in vocabulary.keys():\n",
    "                j = vocabulary[token]\n",
    "                X[i,j] += 1\n",
    "    return X.tocsr()\n",
    "\n",
    "\n",
    "def get_gender(tweet, male_names, female_names):\n",
    "    # Let 1=Female, 0=Male.\n",
    "    name = get_first_name(tweet)\n",
    "    if name in female_names:\n",
    "        return 1\n",
    "    elif name in male_names:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def do_cross_val(X, y, nfolds=5):\n",
    "    \"\"\"\n",
    "    Compute the average testing accuracy over k folds of cross-validation.\n",
    "    Params:\n",
    "      X........A csr_matrix of features.\n",
    "      y........The true labels for each instance in X\n",
    "      nfolds...The number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "      The average testing accuracy of the classifier\n",
    "      over each fold of cross-validation.\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=nfolds, random_state=42, shuffle=True)\n",
    "    accuracies = []\n",
    "    for train_idx, test_idx in cv.split(X):\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        predicted = clf.predict(X[test_idx])\n",
    "        acc = accuracy_score(y[test_idx], predicted)\n",
    "        accuracies.append(acc)\n",
    "    avg = np.mean(accuracies)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def run_all(tweets, labels, use_descr=True, lowercase=True, keep_punctuation=True, descr_prefix=None,\n",
    "            collapse_urls=True, collapse_mentions=True):\n",
    "    \n",
    "    tokens_list = [tweet2tokens(t, use_descr, lowercase, keep_punctuation, descr_prefix,\n",
    "                            collapse_urls, collapse_mentions) for t in tweets]\n",
    "    vocabulary = make_vocabulary(tokens_list)\n",
    "    X = make_feature_matrix(tweets, tokens_list, vocabulary)\n",
    "    acc = do_cross_val(X, labels, 5)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def eval_all_combinations(tweets, labels, options_iter, argnames):\n",
    "    results = []\n",
    "    for options in option_iter:\n",
    "        acc = run_all(tweets, labels, *options)\n",
    "        options_parsed_dict = {name: opt for name, opt in zip(argnames, options)}\n",
    "        results.append((acc, options_parsed_dict))\n",
    "    results_sorted = sorted(results, key=lambda x: -x[0])\n",
    "    return results_sorted\n",
    "\n",
    "\n",
    "def plot_sorted_accuracies(results, filename):\n",
    "    \"\"\"\n",
    "    Plot all accuracies from the result of eval_all_combinations\n",
    "    in ascending order of accuracy.\n",
    "    Save to \"accuracies.png\".\n",
    "    \"\"\"\n",
    "    accuracies = [res_tuple[0] for res_tuple in results]\n",
    "    plt.figure()\n",
    "    plt.plot(sorted(accuracies))\n",
    "    plt.xlabel('setting')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    # for r in sorted(results, reverse=True):\n",
    "    #     print('%.4f' % r[0], '  '.join('%s=%s' % (name, opt) for name, opt in zip(argnames, r[1])))\n",
    "\n",
    "\n",
    "def mean_accuracy_per_setting(results):\n",
    "    \"\"\"\n",
    "    To determine how important each model setting is to overall accuracy,\n",
    "    we'll compute the mean accuracy of all combinations with a particular\n",
    "    setting.\n",
    "\n",
    "    Params:\n",
    "      results...The output of eval_all_combinations\n",
    "    Returns:\n",
    "      A list of (accuracy, setting) tuples, SORTED in\n",
    "      descending order of accuracy.\n",
    "    \"\"\"\n",
    "    appearances = defaultdict(lambda: 0)\n",
    "    setting_total = Counter()\n",
    "    \n",
    "    for result_tuple in results:\n",
    "        accuracy = result_tuple[0]\n",
    "        for key,value in result_tuple[1].items():\n",
    "            if key != 'accuracy':\n",
    "                appearances[\"%s=%s\" % (key,value)] += accuracy\n",
    "                setting_total.update([\"%s=%s\" % (key,value)])\n",
    "    \n",
    "    for key,value in appearances.items():\n",
    "        appearances[key] = value/setting_total[key]\n",
    "        \n",
    "    ret = [(value, key) for key,value in appearances.items()]\n",
    "    \n",
    "    return sorted(ret, key=lambda x:-x[0])\n",
    "\n",
    "\n",
    "def fit_best_classifier(tweets, labels, best_result):\n",
    "    \"\"\"\n",
    "    Using the best setting from eval_all_combinations,\n",
    "    re-vectorize all the training data and fit a\n",
    "    LogisticRegression classifier to all training data.\n",
    "    (i.e., no cross-validation done here)\n",
    "\n",
    "    Params:\n",
    "      docs..........List of training document strings.\n",
    "      labels........The true labels for each training document (0 or 1)\n",
    "      best_result...Element of eval_all_combinations\n",
    "                    with highest accuracy\n",
    "    Returns:\n",
    "      clf.....A LogisticRegression classifier fit to all\n",
    "            training data.\n",
    "      vocab...The dict from feature name to column index.\n",
    "    \"\"\"\n",
    "    tokens_list = [tweet2tokens(t, best_result[1]['use_descr'], best_result[1]['lower'], best_result[1]['punct'], \n",
    "                                best_result[1]['prefix'], best_result[1]['url'], best_result[1]['mention']) for t in tweets]\n",
    "    vocabulary = make_vocabulary(tokens_list)\n",
    "    X = make_feature_matrix(tweets, tokens_list, vocabulary)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, labels)\n",
    "    return model, vocabulary\n",
    "    \n",
    "\n",
    "def top_coefs(clf, label, n, vocab):\n",
    "    \"\"\"\n",
    "    Find the n features with the highest coefficients in\n",
    "    this classifier for this label.\n",
    "    See the .coef_ attribute of LogisticRegression.\n",
    "\n",
    "    Params:\n",
    "      clf.....LogisticRegression classifier\n",
    "      label...1 or 0; if 1, return the top coefficients\n",
    "              for the female class; else for male.\n",
    "      n.......The number of coefficients to return.\n",
    "      vocab...Dict from feature name to column index.\n",
    "    Returns:\n",
    "      List of (feature_name, coefficient) tuples, SORTED\n",
    "      in descending order of the coefficient for the\n",
    "      given class label.\n",
    "    \"\"\"\n",
    "    coef = clf.coef_[0]\n",
    "    if label==1:\n",
    "        # Sort them in descending order.\n",
    "        top_coef_ind = np.argsort(coef)[::-1][:n]\n",
    "    else:\n",
    "        top_coef_ind = np.argsort(coef)[:n]\n",
    "    \n",
    "    # Get the names of those features.\n",
    "    idx2word = dict((v,k) for k,v in vocab.items())\n",
    "    top_coef_terms = [idx2word[i] for i in top_coef_ind]\n",
    "    # Get the weights of those features\n",
    "    top_coef = coef[top_coef_ind]\n",
    "    return [x for x in zip(top_coef_terms, top_coef)]\n",
    "\n",
    "\n",
    "def get_test_gender(tweet):\n",
    "    \"\"\"\n",
    "    Get the gender stored in a tweet\n",
    "    \"\"\"\n",
    "    return tweet['user']['gender']\n",
    "    \n",
    "    \n",
    "def parse_test_data(best_result, vocabulary):\n",
    "    \"\"\"\n",
    "    Using the vocabulary fit to the training data, read\n",
    "    and vectorize the testing data. Note that vocab should\n",
    "    be passed to the vectorize function to ensure the feature\n",
    "    mapping is consistent from training to testing.\n",
    "\n",
    "    Note: use read_data function defined above to read the\n",
    "    test data.\n",
    "\n",
    "    Params:\n",
    "      best_result...Element of eval_all_combinations\n",
    "                    with highest accuracy\n",
    "      vocabulary....dict from feature name to column index,\n",
    "                    built from the training data.\n",
    "    Returns:\n",
    "      test_tweets.....List of strings, one per testing document,\n",
    "                    containing the raw.\n",
    "      test_labels...List of ints, one per testing document,\n",
    "                    1 for positive, 0 for negative.\n",
    "      X_test........A csr_matrix representing the features\n",
    "                    in the test data. Each row is a document,\n",
    "                    each column is a feature.\n",
    "    \"\"\"\n",
    "    test_tweets = pickle.load(open('../data/collect/real-time-tweets-test-dataset.pkl', 'rb'))\n",
    "    test_labels = np.array([get_test_gender(t) for t in test_tweets])\n",
    "    tokens_list = [tweet2tokens(t, best_result[1]['use_descr'], best_result[1]['lower'], best_result[1]['punct'], \n",
    "                                best_result[1]['prefix'], best_result[1]['url'], best_result[1]['mention']) for t in test_tweets]\n",
    "    X_test = make_feature_matrix(test_tweets, tokens_list, vocabulary)    \n",
    "    return test_tweets, test_labels, X_test\n",
    "    \n",
    "\n",
    "def print_top_misclassified(test_tweets, test_labels, X_test, clf, n):\n",
    "    \"\"\"\n",
    "    Print the n testing documents that are misclassified by the\n",
    "    largest margin. By using the .predict_proba function of\n",
    "    LogisticRegression <https://goo.gl/4WXbYA>, we can get the\n",
    "    predicted probabilities of each class for each instance.\n",
    "    We will first identify all incorrectly classified documents,\n",
    "    then sort them in descending order of the predicted probability\n",
    "    for the incorrect class.\n",
    "    E.g., if document i is misclassified as positive, we will\n",
    "    consider the probability of the positive class when sorting.\n",
    "\n",
    "    Params:\n",
    "      test_docs.....List of strings, one per test document\n",
    "      test_labels...Array of true testing labels\n",
    "      X_test........csr_matrix for test data\n",
    "      clf...........LogisticRegression classifier fit on all training\n",
    "                    data.\n",
    "      n.............The number of documents to print.\n",
    "\n",
    "    Returns:\n",
    "      Nothing; see Log.txt for example printed output.\n",
    "    \"\"\"\n",
    "    predictions = clf.predict(X_test)\n",
    "    errors_ind = np.where(test_labels!=predictions)[0]\n",
    "    print(type(errors_ind))\n",
    "    print(errors_ind)\n",
    "    proba_vals = clf.predict_proba(X_test)[errors_ind]\n",
    "    wrong_tweets = []\n",
    "    for idx in errors_ind:\n",
    "        wrong_tweets.append(test_tweets[idx])\n",
    "    predictions = predictions[errors_ind]\n",
    "    test_labels = test_labels[errors_ind]\n",
    "    \n",
    "    proba_vals_class = [elem[0][elem[1]] for elem in zip(proba_vals, predictions)]\n",
    "    \n",
    "    # Sort them in descending order.\n",
    "    sort_ind = np.argsort(proba_vals_class)[::-1][:n]\n",
    "   \n",
    "    for i in sort_ind:\n",
    "        wrong_tweet = wrong_tweets[i]\n",
    "        print(\"truth=%d predicted=%d proba=%6f \\n\" % (test_labels[i], predictions[i], proba_vals_class[i], wrong_tweet['user']['name'], wrong_tweet['user']['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4014 female and 1146 male names\n"
     ]
    }
   ],
   "source": [
    "# 0 - Establish twitter connection and read all the names picked from the U.S. census.\n",
    "male_names, female_names = read_census_names()\n",
    "print('found %d female and %d male names' % (len(female_names), len(male_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled 5000 tweets\n",
      "top names: [('john', 74), ('michael', 60), ('chris', 52), ('mike', 47), ('kevin', 47), ('james', 46), ('ryan', 42), ('jeff', 41), ('david', 38), ('brian', 36)]\n"
     ]
    }
   ],
   "source": [
    "# 1 - Retrieve the real time tweets\n",
    "filename = '../data/collect/real-time-tweets.pkl'\n",
    "tweets = read_real_time_tweets(filename)\n",
    "print('sampled %d tweets' % len(tweets))\n",
    "print('top names:', Counter(get_first_name(t) for t in tweets).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 30498)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agericke/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 35285)\n",
      "(5000, 32522)\n",
      "(5000, 37309)\n",
      "(5000, 26767)\n",
      "(5000, 31493)\n",
      "(5000, 28792)\n",
      "(5000, 33518)\n",
      "(5000, 20426)\n",
      "(5000, 25029)\n",
      "(5000, 22499)\n",
      "(5000, 27098)\n",
      "(5000, 16644)\n",
      "(5000, 21134)\n",
      "(5000, 18707)\n",
      "(5000, 23187)\n",
      "(5000, 34827)\n",
      "(5000, 39632)\n",
      "(5000, 36851)\n",
      "(5000, 41656)\n",
      "(5000, 30925)\n",
      "(5000, 35680)\n",
      "(5000, 32950)\n",
      "(5000, 37705)\n",
      "(5000, 25012)\n",
      "(5000, 29680)\n",
      "(5000, 27120)\n",
      "(5000, 31787)\n",
      "(5000, 20913)\n",
      "(5000, 25487)\n",
      "(5000, 23003)\n",
      "(5000, 27573)\n",
      "(5000, 14067)\n",
      "(5000, 17661)\n",
      "(5000, 15883)\n",
      "(5000, 19477)\n",
      "(5000, 14067)\n",
      "(5000, 17661)\n",
      "(5000, 15883)\n",
      "(5000, 19477)\n",
      "(5000, 9506)\n",
      "(5000, 12977)\n",
      "(5000, 11322)\n",
      "(5000, 14793)\n",
      "(5000, 9506)\n",
      "(5000, 12977)\n",
      "(5000, 11322)\n",
      "(5000, 14793)\n",
      "(5000, 15976)\n",
      "(5000, 19579)\n",
      "(5000, 17792)\n",
      "(5000, 21395)\n",
      "(5000, 15976)\n",
      "(5000, 19579)\n",
      "(5000, 17792)\n",
      "(5000, 21395)\n",
      "(5000, 11563)\n",
      "(5000, 15063)\n",
      "(5000, 13380)\n",
      "(5000, 16880)\n",
      "(5000, 11563)\n",
      "(5000, 15063)\n",
      "(5000, 13380)\n",
      "(5000, 16880)\n"
     ]
    }
   ],
   "source": [
    "# 2 - Obtain accuracies for all possible options\n",
    "labels = np.array([get_gender(t, male_names, female_names) for t in tweets])\n",
    "argnames, option_iter = generate_all_opts()\n",
    "results_sorted  = eval_all_combinations(tweets, labels, option_iter, argnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7482,\n",
       " {'use_descr': True,\n",
       "  'lower': True,\n",
       "  'punct': False,\n",
       "  'prefix': 'd=',\n",
       "  'url': True,\n",
       "  'mention': False})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross-validation result:\n",
      "0.7482\n",
      "with options\n",
      "['use_descr=True', 'lower=True', 'punct=False', 'prefix=d=', 'url=True', 'mention=False']\n",
      "\n",
      "\n",
      "worst cross-validation result:\n",
      "0.6152\n",
      "with options\n",
      "['use_descr=False', 'lower=True', 'punct=False', 'prefix=', 'url=False', 'mention=True']\n"
     ]
    }
   ],
   "source": [
    "# Print information about these results.\n",
    "best_result = results_sorted[0]\n",
    "best_result_opt = []\n",
    "for name, opt in best_result[1].items():\n",
    "    best_result_opt.extend([str(name)+\"=\"+str(opt)])\n",
    "worst_result = results_sorted[-1]\n",
    "worst_result_opt = []\n",
    "for name, opt in worst_result[1].items():\n",
    "    worst_result_opt.extend([str(name)+\"=\"+str(opt)])\n",
    "print('best cross-validation result:\\n%s\\nwith options\\n%s' % (str(best_result[0]), best_result_opt))\n",
    "print(\"\\n\")\n",
    "print('worst cross-validation result:\\n%s\\nwith options\\n%s' % (str(worst_result[0]), worst_result_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = '../images/classify/accuracies.png'\n",
    "plot_sorted_accuracies(results_sorted, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Accuracies per Setting:\n",
      "use_descr=True: 0.73809\n",
      "mention=False: 0.68397\n",
      "prefix=d=: 0.68281\n",
      "lower=True: 0.68209\n",
      "punct=False: 0.68176\n",
      "url=True: 0.68170\n",
      "url=False: 0.68139\n",
      "punct=True: 0.68133\n",
      "lower=False: 0.68099\n",
      "prefix=: 0.68028\n",
      "mention=True: 0.67912\n",
      "use_descr=False: 0.62500\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracies per Setting:')\n",
    "print('\\n'.join(['%s: %.5f' % (s,v) for v,s in mean_accuracy_per_setting(results_sorted)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 25029)\n"
     ]
    }
   ],
   "source": [
    "# Fit best classifier.\n",
    "clf, vocabulary = fit_best_classifier(tweets, labels, best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP COEFFICIENTS PER CLASS:\n",
      "Male tokens:\n",
      "d=father: -1.99182\n",
      "d=husband: -1.57323\n",
      "d=guy: -1.51478\n",
      "d=man: -1.39916\n",
      "d=l: -1.36617\n",
      "\n",
      "Female tokens:\n",
      "d=mom: 2.25591\n",
      "d=mother: 2.08180\n",
      "d=alumna: 1.63699\n",
      "d=she: 1.42975\n",
      "d=girl: 1.38573\n"
     ]
    }
   ],
   "source": [
    "# Print top coefficients per class.\n",
    "print('\\nTOP COEFFICIENTS PER CLASS:')\n",
    "print('Male tokens:')\n",
    "print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 0, 5, vocabulary)]))\n",
    "print('\\nFemale tokens:')\n",
    "print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 1, 5, vocabulary)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 25029)\n"
     ]
    }
   ],
   "source": [
    "# Parse test data\n",
    "test_tweets, test_labels, X_test = parse_test_data(best_result, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test set.\n",
    "predictions = clf.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
      " 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
      " 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1\n",
      " 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0\n",
      " 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
      " 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0\n",
      " 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1\n",
      " 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
      " 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1\n",
      " 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy=0.666000\n"
     ]
    }
   ],
   "source": [
    "print('testing accuracy=%f' % accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP MISCLASSIFIED TEST DOCUMENTS:\n",
      "<class 'numpy.ndarray'>\n",
      "[  0   2  12  18  19  24  28  30  32  36  37  41  51  52  53  54  61  62\n",
      "  74  75  76  81  86  88  98 103 105 107 109 113 118 121 126 137 139 142\n",
      " 143 145 147 150 153 154 156 157 158 159 163 164 168 172 173 174 175 178\n",
      " 180 182 183 185 187 189 190 191 194 195 200 205 206 210 214 221 233 234\n",
      " 236 240 252 257 259 261 262 265 267 269 270 272 273 274 275 278 285 286\n",
      " 288 295 300 303 310 312 314 315 317 318 319 322 325 326 327 328 337 340\n",
      " 343 344 347 348 350 354 356 361 363 367 368 369 372 373 375 378 380 382\n",
      " 385 387 388 390 393 395 396 398 400 406 407 410 411 415 416 419 421 422\n",
      " 426 428 430 431 434 436 445 448 449 454 455 460 463 465 469 473 476 477\n",
      " 487 491 493 494 499]\n",
      "truth=1 predicted=0 proba=0.987003 \n",
      "\n",
      "truth=1 predicted=0 proba=0.986091 \n",
      "\n",
      "truth=1 predicted=0 proba=0.978790 \n",
      "\n",
      "truth=1 predicted=0 proba=0.970207 \n",
      "\n",
      "truth=1 predicted=0 proba=0.960771 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTOP MISCLASSIFIED TEST DOCUMENTS:')\n",
    "error_idx = print_top_misclassified(test_tweets, test_labels, X_test, clf, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8ea61e99a1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
