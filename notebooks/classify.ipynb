{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guessing gender\n",
    "# Collect 1500 tweets matching words related to Blockchain\n",
    "import configparser\n",
    "import sys\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from TwitterAPI import TwitterAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Read census_names and tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4014 female and 1146 male names\n"
     ]
    }
   ],
   "source": [
    "def read_census_names():\n",
    "    \"\"\"\n",
    "    Read census names collected in the collect python script.\n",
    "\n",
    "    Returns:\n",
    "        Two lists of male_names and female_names\n",
    "    \"\"\"\n",
    "    male_names = pickle.load(open('../data/collect/male_names.pkl', 'rb'))\n",
    "    female_names = pickle.load(open('../data/collect/female_names.pkl', 'rb'))\n",
    "    return male_names, female_names\n",
    "\n",
    "# 0 - Establish twitter connection and read all the names picked from the U.S. census.\n",
    "male_names, female_names = read_census_names()\n",
    "print('found %d female and %d male names' % (len(female_names), len(male_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Twitter connection.\n"
     ]
    }
   ],
   "source": [
    "def get_twitter(config_file):\n",
    "    \"\"\" Read the config_file and construct an instance of TwitterAPI.\n",
    "    Args:\n",
    "      config_file ... A config file in ConfigParser format with Twitter credentials\n",
    "    Returns:\n",
    "      An instance of TwitterAPI.\n",
    "    \"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "twitter = get_twitter('../twitter.cfg')\n",
    "print('Established Twitter connection.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_real_time_tweets(filename):\n",
    "    \"\"\"Read real time tweets retrieved during collect phase\n",
    "\n",
    "    Params:\n",
    "        filename.....The file where the tweets are stored.\n",
    "    Returns:\n",
    "        The list of real time tweets\n",
    "    \"\"\"\n",
    "    return pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_name(tweet):\n",
    "    \"\"\"\n",
    "    Get the first name from a twitter object.\n",
    "    \n",
    "    Params:\n",
    "        tweet....The Twitter object from where to pick the user name.\n",
    "    Returns:\n",
    "        The user first name in lower letters.\n",
    "    \"\"\"\n",
    "    if 'user' in tweet and 'name' in tweet['user']:\n",
    "        parts = tweet['user']['name'].split()\n",
    "        if len(parts) > 0:\n",
    "            return parts[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/collect/real-time-tweets.pkl'\n",
    "tweets = read_real_time_tweets(filename)\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled 5000 tweets\n",
      "top names: [('john', 74), ('michael', 60), ('chris', 52), ('mike', 47), ('kevin', 47), ('james', 46), ('ryan', 42), ('jeff', 41), ('david', 38), ('brian', 36)]\n"
     ]
    }
   ],
   "source": [
    "print('sampled %d tweets' % len(tweets))\n",
    "print('top names:', Counter(get_first_name(t) for t in tweets).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test tweet:\n",
      "\tscreen_name=MekaPye100\n",
      "\tname=Tomeka  Dorsey\n",
      "\tdescr=I AM BY ALL MEANS A TENACIOUS INDIVIDUAL!\n",
      "\ttext=@JTthepodcaster It is, my mother made me watch it 1001 times...if they ever remake it you should try out for the part\n",
      "top languages: [('en', 5000)]\n"
     ]
    }
   ],
   "source": [
    "test_tweet = tweets[1]\n",
    "print('test tweet:\\n\\tscreen_name=%s\\n\\tname=%s\\n\\tdescr=%s\\n\\ttext=%s' %\n",
    "      (test_tweet['user']['screen_name'],\n",
    "       test_tweet['user']['name'],\n",
    "       test_tweet['user']['description'],\n",
    "       test_tweet['text']))\n",
    "print('top languages:', Counter(t['lang'] for t in tweets).most_common(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tokenize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(string, lowercase, keep_punctuation, prefix, collapse_urls, collapse_mentions):\n",
    "    \"\"\" \n",
    "    Split a string into tokens.\n",
    "    If keep_internal_punct is False, then return only the alphanumerics (letters, numbers and underscore).\n",
    "    If keep_internal_punct is True, then also retain punctuation that\n",
    "    is inside of a word. E.g., in the example below, the token \"isn't\"\n",
    "    is maintained when keep_internal_punct=True; otherwise, it is\n",
    "    split into \"isn\" and \"t\" tokens\n",
    "    \n",
    "    Params:\n",
    "        string................The string that needs to be tokenized.\n",
    "        lowercase.............Boolean indicating if we want the text to be convert to lowercase.\n",
    "        keep_punctuation......Boolean indicating if we want to keep punctuation\n",
    "        prefix................Prefix to add to each obtained token. (will use for identifying what part is being tokenized, e.g. prefix d= for description)\n",
    "        collapse_urls.........Boolean indicating if we ant to collapse the urls in the text. (e.g. @something)\n",
    "        collapse_meentions....Boolean indicating if we ant to collapse the mmentions in the text. (e.g. #smth)\n",
    "    Returns:\n",
    "        An array containing the tokenized string.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return []\n",
    "    if lowercase:\n",
    "        string = string.lower()\n",
    "    tokens = []\n",
    "    if collapse_urls:\n",
    "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
    "    if collapse_mentions:\n",
    "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
    "    if keep_punctuation:\n",
    "        tokens = string.split()\n",
    "    else:\n",
    "        tokens = re.sub('\\W+', ' ', string).split()\n",
    "    if prefix:\n",
    "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d=i',\n",
       " 'd=am',\n",
       " 'd=by',\n",
       " 'd=all',\n",
       " 'd=means',\n",
       " 'd=a',\n",
       " 'd=tenacious',\n",
       " 'd=individual']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(test_tweet['user']['description'], lowercase=True,\n",
    "         keep_punctuation=False, prefix='d=',\n",
    "         collapse_urls=True, collapse_mentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet2tokens(tweet, use_descr=True, lowercase=True, keep_punctuation=True, descr_prefix='d=', collapse_urls=True, collapse_mentions=True):\n",
    "    \"\"\"\n",
    "    Convert a tweet into a list of tokens, from the tweet text and optionally the\n",
    "    user description.\n",
    "    \n",
    "    Params:\n",
    "        tweet.................The tweet that needs to be tokenized.\n",
    "        user_descr............Boolean to indicate if we want to tokenize the user description too.\n",
    "        lowercase.............Boolean indicating if we want the text to be convert to lowercase.\n",
    "        keep_punctuation......Boolean indicating if we want to keep punctuation\n",
    "        descr_prefix..........Prefix to add to the tokenization of the description.\n",
    "        collapse_urls.........Boolean indicating if we ant to collapse the urls in the text. (e.g. @something)\n",
    "        collapse_meentions....Boolean indicating if we ant to collapse the mmentions in the text. (e.g. #smth)\n",
    "    \"\"\"\n",
    "    # When tokenizing the text, do not add any prefix.\n",
    "    tokens = tokenize(tweet['text'], lowercase, keep_punctuation, None, collapse_urls, collapse_mentions)\n",
    "    if use_descr:\n",
    "        tokens.extend(tokenize(tweet['user']['description'], lowercase, keep_punctuation, descr_prefix,\n",
    "                               collapse_urls, collapse_mentions))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THIS_IS_A_MENTION',\n",
       " 'it',\n",
       " 'is,',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'made',\n",
       " 'me',\n",
       " 'watch',\n",
       " 'it',\n",
       " '1001',\n",
       " 'times...if',\n",
       " 'they',\n",
       " 'ever',\n",
       " 'remake',\n",
       " 'it',\n",
       " 'you',\n",
       " 'should',\n",
       " 'try',\n",
       " 'out',\n",
       " 'for',\n",
       " 'the',\n",
       " 'part',\n",
       " 'd=i',\n",
       " 'd=am',\n",
       " 'd=by',\n",
       " 'd=all',\n",
       " 'd=means',\n",
       " 'd=a',\n",
       " 'd=tenacious',\n",
       " 'd=individual!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet2tokens(test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_descr=True  lower=True  punct=True  prefix=d=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  d=i  d=am  d=by  d=all  d=means  d=a  d=tenacious  d=individual! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=True  prefix=d=  url=True  mention=False\n",
      "@jtthepodcaster  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  d=i  d=am  d=by  d=all  d=means  d=a  d=tenacious  d=individual! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=True  prefix=d=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  d=i  d=am  d=by  d=all  d=means  d=a  d=tenacious  d=individual! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=True  prefix=d=  url=False  mention=False\n",
      "@jtthepodcaster  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  d=i  d=am  d=by  d=all  d=means  d=a  d=tenacious  d=individual! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=True  prefix=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  i  am  by  all  means  a  tenacious  individual! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=True  prefix=  url=True  mention=False\n",
      "@jtthepodcaster  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  i  am  by  all  means  a  tenacious  individual! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=True  prefix=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  i  am  by  all  means  a  tenacious  individual! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=True  prefix=  url=False  mention=False\n",
      "@jtthepodcaster  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  i  am  by  all  means  a  tenacious  individual! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=False  prefix=d=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  d=i  d=am  d=by  d=all  d=means  d=a  d=tenacious  d=individual \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=False  prefix=d=  url=True  mention=False\n",
      "jtthepodcaster  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  d=i  d=am  d=by  d=all  d=means  d=a  d=tenacious  d=individual \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=False  prefix=d=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  d=i  d=am  d=by  d=all  d=means  d=a  d=tenacious  d=individual \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=False  prefix=d=  url=False  mention=False\n",
      "jtthepodcaster  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  d=i  d=am  d=by  d=all  d=means  d=a  d=tenacious  d=individual \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=False  prefix=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  i  am  by  all  means  a  tenacious  individual \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=False  prefix=  url=True  mention=False\n",
      "jtthepodcaster  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  i  am  by  all  means  a  tenacious  individual \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=False  prefix=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  i  am  by  all  means  a  tenacious  individual \n",
      "----\n",
      "\n",
      "use_descr=True  lower=True  punct=False  prefix=  url=False  mention=False\n",
      "jtthepodcaster  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  i  am  by  all  means  a  tenacious  individual \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=True  prefix=d=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  d=I  d=AM  d=BY  d=ALL  d=MEANS  d=A  d=TENACIOUS  d=INDIVIDUAL! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=True  prefix=d=  url=True  mention=False\n",
      "@JTthepodcaster  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  d=I  d=AM  d=BY  d=ALL  d=MEANS  d=A  d=TENACIOUS  d=INDIVIDUAL! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=True  prefix=d=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  d=I  d=AM  d=BY  d=ALL  d=MEANS  d=A  d=TENACIOUS  d=INDIVIDUAL! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=True  prefix=d=  url=False  mention=False\n",
      "@JTthepodcaster  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  d=I  d=AM  d=BY  d=ALL  d=MEANS  d=A  d=TENACIOUS  d=INDIVIDUAL! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=True  prefix=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  I  AM  BY  ALL  MEANS  A  TENACIOUS  INDIVIDUAL! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=True  prefix=  url=True  mention=False\n",
      "@JTthepodcaster  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  I  AM  BY  ALL  MEANS  A  TENACIOUS  INDIVIDUAL! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=True  prefix=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  I  AM  BY  ALL  MEANS  A  TENACIOUS  INDIVIDUAL! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=True  prefix=  url=False  mention=False\n",
      "@JTthepodcaster  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part  I  AM  BY  ALL  MEANS  A  TENACIOUS  INDIVIDUAL! \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=False  prefix=d=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  d=I  d=AM  d=BY  d=ALL  d=MEANS  d=A  d=TENACIOUS  d=INDIVIDUAL \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=False  prefix=d=  url=True  mention=False\n",
      "JTthepodcaster  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  d=I  d=AM  d=BY  d=ALL  d=MEANS  d=A  d=TENACIOUS  d=INDIVIDUAL \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=False  prefix=d=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  d=I  d=AM  d=BY  d=ALL  d=MEANS  d=A  d=TENACIOUS  d=INDIVIDUAL \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=False  prefix=d=  url=False  mention=False\n",
      "JTthepodcaster  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  d=I  d=AM  d=BY  d=ALL  d=MEANS  d=A  d=TENACIOUS  d=INDIVIDUAL \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=False  prefix=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  I  AM  BY  ALL  MEANS  A  TENACIOUS  INDIVIDUAL \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=False  prefix=  url=True  mention=False\n",
      "JTthepodcaster  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  I  AM  BY  ALL  MEANS  A  TENACIOUS  INDIVIDUAL \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=False  prefix=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  I  AM  BY  ALL  MEANS  A  TENACIOUS  INDIVIDUAL \n",
      "----\n",
      "\n",
      "use_descr=True  lower=False  punct=False  prefix=  url=False  mention=False\n",
      "JTthepodcaster  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part  I  AM  BY  ALL  MEANS  A  TENACIOUS  INDIVIDUAL \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=True  prefix=d=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=True  prefix=d=  url=True  mention=False\n",
      "@jtthepodcaster  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=True  prefix=d=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=True  prefix=d=  url=False  mention=False\n",
      "@jtthepodcaster  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=True  prefix=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=True  prefix=  url=True  mention=False\n",
      "@jtthepodcaster  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=True  prefix=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=True  prefix=  url=False  mention=False\n",
      "@jtthepodcaster  it  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=False  prefix=d=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=False  prefix=d=  url=True  mention=False\n",
      "jtthepodcaster  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=False  prefix=d=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=False  prefix=d=  url=False  mention=False\n",
      "jtthepodcaster  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=False  prefix=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=False  prefix=  url=True  mention=False\n",
      "jtthepodcaster  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=False  prefix=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=True  punct=False  prefix=  url=False  mention=False\n",
      "jtthepodcaster  it  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=True  prefix=d=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=True  prefix=d=  url=True  mention=False\n",
      "@JTthepodcaster  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=True  prefix=d=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=True  prefix=d=  url=False  mention=False\n",
      "@JTthepodcaster  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=True  prefix=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=True  prefix=  url=True  mention=False\n",
      "@JTthepodcaster  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=True  prefix=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=True  prefix=  url=False  mention=False\n",
      "@JTthepodcaster  It  is,  my  mother  made  me  watch  it  1001  times...if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=False  prefix=d=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=False  prefix=d=  url=True  mention=False\n",
      "JTthepodcaster  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=False  prefix=d=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=False  prefix=d=  url=False  mention=False\n",
      "JTthepodcaster  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=False  prefix=  url=True  mention=True\n",
      "THIS_IS_A_MENTION  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=False  prefix=  url=True  mention=False\n",
      "JTthepodcaster  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=False  prefix=  url=False  mention=True\n",
      "THIS_IS_A_MENTION  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n",
      "use_descr=False  lower=False  punct=False  prefix=  url=False  mention=False\n",
      "JTthepodcaster  It  is  my  mother  made  me  watch  it  1001  times  if  they  ever  remake  it  you  should  try  out  for  the  part \n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for enumerating all possible arguments of tweet2tokens\n",
    "# https://docs.python.org/2/library/itertools.html#itertools.product\n",
    "from itertools import product\n",
    "\n",
    "use_descr_opts = [True, False]\n",
    "lowercase_opts = [True, False]\n",
    "keep_punctuation_opts = [True, False]\n",
    "descr_prefix_opts = ['d=', '']\n",
    "url_opts = [True, False]\n",
    "mention_opts = [True, False]\n",
    "\n",
    "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention']\n",
    "option_iter = product(use_descr_opts, lowercase_opts,\n",
    "                       keep_punctuation_opts,\n",
    "                       descr_prefix_opts, url_opts,\n",
    "                       mention_opts)\n",
    "for options in option_iter:\n",
    "    print('  '.join('%s=%s' % (name, opt) \n",
    "                    for name, opt in zip(argnames, options)))\n",
    "    print\n",
    "    print('  '.join(tweet2tokens(test_tweet, *options)), '\\n----\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THIS_IS_A_MENTION',\n",
       " 'it',\n",
       " 'is',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'made',\n",
       " 'me',\n",
       " 'watch',\n",
       " 'it',\n",
       " '1001',\n",
       " 'times',\n",
       " 'if',\n",
       " 'they',\n",
       " 'ever',\n",
       " 'remake',\n",
       " 'it',\n",
       " 'you',\n",
       " 'should',\n",
       " 'try',\n",
       " 'out',\n",
       " 'for',\n",
       " 'the',\n",
       " 'part',\n",
       " 'd=i',\n",
       " 'd=am',\n",
       " 'd=by',\n",
       " 'd=all',\n",
       " 'd=means',\n",
       " 'd=a',\n",
       " 'd=tenacious',\n",
       " 'd=individual']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's tokenize all tweets.\n",
    "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
    "                            keep_punctuation=False, descr_prefix='d=',\n",
    "                            collapse_urls=True, collapse_mentions=True)\n",
    "              for t in tweets]\n",
    "tokens_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store these in a sparse matrix.\n",
    "\n",
    "#1) Create a vocabulary (dict from term->index)\n",
    "\n",
    "# https://docs.python.org/2/library/collections.html#collections.defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "def make_vocabulary(tokens_list):\n",
    "    vocabulary = defaultdict(lambda: len(vocabulary))  # If term not present, assign next int.\n",
    "    for tokens in tokens_list:\n",
    "        for token in tokens:\n",
    "            vocabulary[token]  # looking up a key; defaultdict takes care of assigning it a value.\n",
    "    print('%d unique terms in vocabulary' % len(vocabulary))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20426 unique terms in vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('told', 0),\n",
       " ('myself', 1),\n",
       " ('i', 2),\n",
       " ('was', 3),\n",
       " ('gonna', 4),\n",
       " ('get', 5),\n",
       " ('healthy', 6),\n",
       " ('but', 7),\n",
       " ('m', 8),\n",
       " ('at', 9)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = make_vocabulary(tokens_list)\n",
    "# term->index\n",
    "list(vocabulary.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30498 unique terms in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# How big is vocabulary if we keep punctuation?\n",
    "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
    "                            keep_punctuation=True, descr_prefix='d=',\n",
    "                            collapse_urls=True, collapse_mentions=True)\n",
    "              for t in tweets]\n",
    "\n",
    "vocabulary = make_vocabulary(tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Convert tokenized tweets into CSR matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to a sparse matrix X.\n",
    "# X[i,j] is the frequency of term j in tweet i\n",
    "# \n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def make_feature_matrix(tokens_list, vocabulary):\n",
    "    X = lil_matrix((len(tweets), len(vocabulary)))\n",
    "    for i, tokens in enumerate(tokens_list):\n",
    "        for token in tokens:\n",
    "            j = vocabulary[token]\n",
    "            X[i,j] += 1\n",
    "    return X.tocsr()  # convert to CSR for more efficient random access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (5000, 30498)\n"
     ]
    }
   ],
   "source": [
    "X = make_feature_matrix(tokens_list, vocabulary)\n",
    "print('shape of X:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x30498 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How is tweet stored?\n",
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-zero indices of terms used in tweet 1.\n",
    "X[1].nonzero()[1]  # col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.make_vocabulary.<locals>.<lambda>()>,\n",
       "            {'told': 0,\n",
       "             'myself': 1,\n",
       "             'i': 2,\n",
       "             'was': 3,\n",
       "             'gonna': 4,\n",
       "             'get': 5,\n",
       "             'healthy': 6,\n",
       "             'but': 7,\n",
       "             'i’m': 8,\n",
       "             'at': 9,\n",
       "             'whataburger': 10,\n",
       "             'rn': 11,\n",
       "             '😌': 12,\n",
       "             'd=i': 13,\n",
       "             'd=prefer': 14,\n",
       "             'd=to': 15,\n",
       "             'd=get': 16,\n",
       "             'd=high': 17,\n",
       "             'd=on': 18,\n",
       "             'd=life': 19,\n",
       "             'THIS_IS_A_MENTION': 20,\n",
       "             'it': 21,\n",
       "             'is,': 22,\n",
       "             'my': 23,\n",
       "             'mother': 24,\n",
       "             'made': 25,\n",
       "             'me': 26,\n",
       "             'watch': 27,\n",
       "             '1001': 28,\n",
       "             'times...if': 29,\n",
       "             'they': 30,\n",
       "             'ever': 31,\n",
       "             'remake': 32,\n",
       "             'you': 33,\n",
       "             'should': 34,\n",
       "             'try': 35,\n",
       "             'out': 36,\n",
       "             'for': 37,\n",
       "             'the': 38,\n",
       "             'part': 39,\n",
       "             'd=am': 40,\n",
       "             'd=by': 41,\n",
       "             'd=all': 42,\n",
       "             'd=means': 43,\n",
       "             'd=a': 44,\n",
       "             'd=tenacious': 45,\n",
       "             'd=individual!': 46,\n",
       "             'she': 47,\n",
       "             'deserves': 48,\n",
       "             '100': 49,\n",
       "             'putting': 50,\n",
       "             'up': 51,\n",
       "             'with': 52,\n",
       "             'you!😜': 53,\n",
       "             'congratulations': 54,\n",
       "             'on': 55,\n",
       "             'a': 56,\n",
       "             'great': 57,\n",
       "             'season': 58,\n",
       "             'quinton!!!': 59,\n",
       "             'd=principal,': 60,\n",
       "             'd=st.': 61,\n",
       "             'd=maximilian': 62,\n",
       "             'd=kolbe': 63,\n",
       "             'd=catholic': 64,\n",
       "             'd=school,': 65,\n",
       "             'd=aurora,': 66,\n",
       "             'd=ontario.': 67,\n",
       "             'd=check': 68,\n",
       "             'd=in': 69,\n",
       "             'd=for': 70,\n",
       "             'd=tweets': 71,\n",
       "             'd=about': 72,\n",
       "             'd=education,': 73,\n",
       "             'd=the': 74,\n",
       "             'd=principal’s': 75,\n",
       "             'd=chair': 76,\n",
       "             'd=and': 77,\n",
       "             'd=things': 78,\n",
       "             'd=kolbe!!': 79,\n",
       "             '“the': 80,\n",
       "             'lord': 81,\n",
       "             'gives,': 82,\n",
       "             'and': 83,\n",
       "             'takes': 84,\n",
       "             'away.': 85,\n",
       "             'blessed': 86,\n",
       "             'be': 87,\n",
       "             'name': 88,\n",
       "             'of': 89,\n",
       "             'lord.”': 90,\n",
       "             'd=vp,': 91,\n",
       "             'd=communications': 92,\n",
       "             'd=THIS_IS_A_MENTION': 93,\n",
       "             'd=wife:': 94,\n",
       "             'd=linda.': 95,\n",
       "             'd=kids:': 96,\n",
       "             'd=lauren': 97,\n",
       "             'd=(&': 98,\n",
       "             'd=chris),': 99,\n",
       "             'd=andrew.': 100,\n",
       "             'd=pater': 101,\n",
       "             'd=ben': 102,\n",
       "             'd=&': 103,\n",
       "             'd=stella.': 104,\n",
       "             'd=sbc': 105,\n",
       "             'd=minister.': 106,\n",
       "             'd=#httr': 107,\n",
       "             'd=ن': 108,\n",
       "             'not': 109,\n",
       "             'everyone!!!!!...': 110,\n",
       "             'anyone': 111,\n",
       "             'here': 112,\n",
       "             'who': 113,\n",
       "             'is': 114,\n",
       "             'crypto': 115,\n",
       "             'trader': 116,\n",
       "             'literally': 117,\n",
       "             \"haven't\": 118,\n",
       "             'earned': 119,\n",
       "             'dm...': 120,\n",
       "             'als…': 121,\n",
       "             'THIS_IS_A_URL': 122,\n",
       "             'd=risk': 123,\n",
       "             'd=is': 124,\n",
       "             'd=price': 125,\n",
       "             'd=you': 126,\n",
       "             'd=pay': 127,\n",
       "             'd=opportunity': 128,\n",
       "             'd=presented': 129,\n",
       "             'd=you,if': 130,\n",
       "             'd=your': 131,\n",
       "             'd=current': 132,\n",
       "             'd=source': 133,\n",
       "             'd=of': 134,\n",
       "             'd=income': 135,\n",
       "             'd=can': 136,\n",
       "             'd=secure': 137,\n",
       "             \"d=children's\": 138,\n",
       "             'd=future': 139,\n",
       "             'd=then': 140,\n",
       "             'd=ignore....': 141,\n",
       "             'couple': 142,\n",
       "             'wildlings': 143,\n",
       "             'invading': 144,\n",
       "             'militia': 145,\n",
       "             'hockey!': 146,\n",
       "             '-': 147,\n",
       "             '#hockey': 148,\n",
       "             '#icehockey': 149,\n",
       "             '#hockeylife': 150,\n",
       "             '#hockeyplayer…': 151,\n",
       "             'party': 152,\n",
       "             'dj': 153,\n",
       "             'el': 154,\n",
       "             'sid': 155,\n",
       "             '🎥🍿🔥': 156,\n",
       "             '—': 157,\n",
       "             'cure': 158,\n",
       "             'lounge': 159,\n",
       "             'd=international': 160,\n",
       "             'd=club': 161,\n",
       "             'd=dj': 162,\n",
       "             'd=from': 163,\n",
       "             'd=boston.': 164,\n",
       "             'd=versatile': 165,\n",
       "             'd=mobile': 166,\n",
       "             'd=events': 167,\n",
       "             'd=official': 168,\n",
       "             'd=mixshow': 169,\n",
       "             'd=jamn': 170,\n",
       "             'd=94.5': 171,\n",
       "             'd=fm': 172,\n",
       "             'd=website:': 173,\n",
       "             'd=THIS_IS_A_URL': 174,\n",
       "             'go': 175,\n",
       "             'stupid,': 176,\n",
       "             'crazy': 177,\n",
       "             'd=student-athlete': 178,\n",
       "             'd=@': 179,\n",
       "             'd=calera': 180,\n",
       "             'd=school': 181,\n",
       "             'd=commit': 182,\n",
       "             'd=🐘❤️': 183,\n",
       "             'd=#wearebirmingham': 184,\n",
       "             'celebration': 185,\n",
       "             'our': 186,\n",
       "             'collaborative': 187,\n",
       "             'inquiry': 188,\n",
       "             'group': 189,\n",
       "             'today.': 190,\n",
       "             '27': 191,\n",
       "             'sessions': 192,\n",
       "             'dedication': 193,\n",
       "             'to': 194,\n",
       "             'growing': 195,\n",
       "             'practicein': 196,\n",
       "             'order': 197,\n",
       "             'to…': 198,\n",
       "             'd=administrative': 199,\n",
       "             'd=intern': 200,\n",
       "             'd=at': 201,\n",
       "             'd=ellis': 202,\n",
       "             'd=elementary.': 203,\n",
       "             'd=fighting': 204,\n",
       "             'd=children': 205,\n",
       "             'd=because': 206,\n",
       "             'd=they': 207,\n",
       "             'd=deserve': 208,\n",
       "             'd=best.': 209,\n",
       "             'wonder': 210,\n",
       "             'if': 211,\n",
       "             \"they're\": 212,\n",
       "             'any': 213,\n",
       "             'good?': 214,\n",
       "             'anyways...': 215,\n",
       "             \"it's\": 216,\n",
       "             'like': 217,\n",
       "             'have': 218,\n",
       "             'competition.': 219,\n",
       "             '😏': 220,\n",
       "             'good': 221,\n",
       "             'taste': 222,\n",
       "             'd=cat': 223,\n",
       "             'd=mom:': 224,\n",
       "             'd=misti.': 225,\n",
       "             'd=nichols': 226,\n",
       "             'd=college.': 227,\n",
       "             'd=green': 228,\n",
       "             'd=day.': 229,\n",
       "             'd=youtube': 230,\n",
       "             'd=addict.': 231,\n",
       "             'd=whovian.': 232,\n",
       "             'd=fun': 233,\n",
       "             'd=fact:': 234,\n",
       "             'd=followed': 235,\n",
       "             'd=liam': 236,\n",
       "             'd=payne': 237,\n",
       "             'd=luke': 238,\n",
       "             'd=hemmings.': 239,\n",
       "             'd=instagram:': 240,\n",
       "             'd=amysposito': 241,\n",
       "             'd=snapchat:': 242,\n",
       "             'd=amy010190': 243,\n",
       "             \"let's\": 244,\n",
       "             'some': 245,\n",
       "             'den': 246,\n",
       "             \"d=what's\": 247,\n",
       "             'd=with': 248,\n",
       "             'd=out': 249,\n",
       "             'd=little': 250,\n",
       "             'd=#ftown': 251,\n",
       "             'd=#broncos': 252,\n",
       "             'd=#colorado': 253,\n",
       "             'd=#rattpack': 254,\n",
       "             'd=ig:alwaysamexican': 255,\n",
       "             'd=sc:same': 256,\n",
       "             'd=as': 257,\n",
       "             'd=#usnavy': 258,\n",
       "             'd=#sandiego': 259,\n",
       "             '#walkaway': 260,\n",
       "             \"there's\": 261,\n",
       "             'that': 262,\n",
       "             'dictator': 263,\n",
       "             '..': 264,\n",
       "             'giving': 265,\n",
       "             'choices': 266,\n",
       "             'people...again...': 267,\n",
       "             'boy': 268,\n",
       "             'oh': 269,\n",
       "             \"boy...he's\": 270,\n",
       "             'forcing…': 271,\n",
       "             'd=g.r.i.t.s.': 272,\n",
       "             'd=4trump': 273,\n",
       "             'd=recovered': 274,\n",
       "             'd=long': 275,\n",
       "             'd=democrat': 276,\n",
       "             'd=!': 277,\n",
       "             'd=#obamadidntcare': 278,\n",
       "             'd=#walkaway': 279,\n",
       "             'd=job': 280,\n",
       "             'd=33:15-17': 281,\n",
       "             'd=pray': 282,\n",
       "             'd=congress!': 283,\n",
       "             'd=📖': 284,\n",
       "             'these': 285,\n",
       "             'boot': 286,\n",
       "             'camp': 287,\n",
       "             'classes': 288,\n",
       "             'are': 289,\n",
       "             'kicking': 290,\n",
       "             'ass': 291,\n",
       "             'can’t': 292,\n",
       "             'wait': 293,\n",
       "             'back': 294,\n",
       "             'again': 295,\n",
       "             'tomorrow': 296,\n",
       "             'lol': 297,\n",
       "             'd=🌙': 298,\n",
       "             'd=❁🇲🇽': 299,\n",
       "             '“i': 300,\n",
       "             'wanna': 301,\n",
       "             'in': 302,\n",
       "             'this': 303,\n",
       "             'show': 304,\n",
       "             'so': 305,\n",
       "             'bad.”': 306,\n",
       "             '-jessie,': 307,\n",
       "             'talking': 308,\n",
       "             'about': 309,\n",
       "             'got': 310,\n",
       "             'd=just': 311,\n",
       "             'd=here': 312,\n",
       "             'd=have': 313,\n",
       "             'd=good': 314,\n",
       "             'd=time': 315,\n",
       "             'd=#umich': 316,\n",
       "             'd=❣': 317,\n",
       "             'what': 318,\n",
       "             'you’re': 319,\n",
       "             'saying,': 320,\n",
       "             'agree': 321,\n",
       "             'an': 322,\n",
       "             'extent.': 323,\n",
       "             'however,': 324,\n",
       "             'we': 325,\n",
       "             'don’t': 326,\n",
       "             'outright': 327,\n",
       "             'trash': 328,\n",
       "             'players': 329,\n",
       "             'or': 330,\n",
       "             'team…': 331,\n",
       "             'd=sports': 332,\n",
       "             'd=editor': 333,\n",
       "             'd=#ung': 334,\n",
       "             'd=alum.': 335,\n",
       "             'd=views': 336,\n",
       "             'd=are': 337,\n",
       "             'd=my': 338,\n",
       "             'd=own.': 339,\n",
       "             'd=if': 340,\n",
       "             'd=see': 341,\n",
       "             'd=typo': 342,\n",
       "             'd=tweet': 343,\n",
       "             'd=it’s': 344,\n",
       "             'd=probably': 345,\n",
       "             'd=autocorrect’s': 346,\n",
       "             'd=fault.': 347,\n",
       "             'ha': 348,\n",
       "             'no,': 349,\n",
       "             'i’ll': 350,\n",
       "             'there': 351,\n",
       "             '10': 352,\n",
       "             'days': 353,\n",
       "             'd=trad.': 354,\n",
       "             '16': 355,\n",
       "             'minutes': 356,\n",
       "             'haven’t': 357,\n",
       "             'snapped': 358,\n",
       "             'back...': 359,\n",
       "             'unacceptable': 360,\n",
       "             'd=☀️♏️🌙♌️🖕🏻♉️': 361,\n",
       "             'married': 362,\n",
       "             'sum?': 363,\n",
       "             'd=army.': 364,\n",
       "             'd=🔝racing🔝': 365,\n",
       "             'd=driver': 366,\n",
       "             'd=#2': 367,\n",
       "             'd=knoxville': 368,\n",
       "             'd=raceway.': 369,\n",
       "             'd=self': 370,\n",
       "             'd=proclaimed': 371,\n",
       "             'd=fan': 372,\n",
       "             'd=favorite.': 373,\n",
       "             'from': 374,\n",
       "             'box': 375,\n",
       "             'color': 376,\n",
       "             'blonde': 377,\n",
       "             'bombshell!': 378,\n",
       "             '#tghairlove': 379,\n",
       "             '#hairgoals': 380,\n",
       "             '#redkenobsessed': 381,\n",
       "             '#glossitup': 382,\n",
       "             '#suitelife': 383,\n",
       "             '#lighterforsummer…': 384,\n",
       "             'd=hair': 385,\n",
       "             'd=stylist': 386,\n",
       "             'd=follow': 387,\n",
       "             'd=me': 388,\n",
       "             'had': 389,\n",
       "             'drink': 390,\n",
       "             'over': 391,\n",
       "             '2': 392,\n",
       "             'months.': 393,\n",
       "             'pretty': 394,\n",
       "             'sad': 395,\n",
       "             'admit': 396,\n",
       "             'longest': 397,\n",
       "             'sobriety': 398,\n",
       "             \"i've\": 399,\n",
       "             'achieved': 400,\n",
       "             'since': 401,\n",
       "             'college.': 402,\n",
       "             'getting': 403,\n",
       "             'old': 404,\n",
       "             'weird.': 405,\n",
       "             'd=hawkeyes,': 406,\n",
       "             'd=cubs,': 407,\n",
       "             'd=blackhawks.': 408,\n",
       "             'd=iowa': 409,\n",
       "             'd=native,': 410,\n",
       "             'd=but': 411,\n",
       "             'd=father': 412,\n",
       "             'd=2': 413,\n",
       "             'd=genuine': 414,\n",
       "             'd=chicago': 415,\n",
       "             'd=kids.': 416,\n",
       "             'd=u': 417,\n",
       "             'd=class': 418,\n",
       "             'd=2000.': 419,\n",
       "             'd=usually': 420,\n",
       "             'd=sports.': 421,\n",
       "             'd=funny': 422,\n",
       "             'd=stuff.': 423,\n",
       "             'role': 424,\n",
       "             'cryptocurrency': 425,\n",
       "             'crime': 426,\n",
       "             'darknet': 427,\n",
       "             'activity': 428,\n",
       "             'soars': 429,\n",
       "             'nulltx': 430,\n",
       "             'd=crypto-nut': 431,\n",
       "             'd=an': 432,\n",
       "             'd=interest': 433,\n",
       "             'd=crypto': 434,\n",
       "             'd=research': 435,\n",
       "             'd=investment': 436,\n",
       "             'all': 437,\n",
       "             'democratic': 438,\n",
       "             'has': 439,\n",
       "             'done': 440,\n",
       "             'president': 441,\n",
       "             'two': 442,\n",
       "             'years,': 443,\n",
       "             'will': 444,\n",
       "             'do': 445,\n",
       "             'voting': 446,\n",
       "             'machines!': 447,\n",
       "             'd=deplorable.': 448,\n",
       "             'd=only': 449,\n",
       "             'd=chat': 450,\n",
       "             'd=politics,': 451,\n",
       "             'd=not': 452,\n",
       "             'd=interested': 453,\n",
       "             'd=anything': 454,\n",
       "             'd=else.': 455,\n",
       "             'currently': 456,\n",
       "             'life': 457,\n",
       "             '😂😂😂😂': 458,\n",
       "             'd=super': 459,\n",
       "             'd=robbie': 460,\n",
       "             'd=save': 461,\n",
       "             'd=world': 462,\n",
       "             'd=his': 463,\n",
       "             'd=daily': 464,\n",
       "             'd=rants': 465,\n",
       "             'd=utter': 466,\n",
       "             'd=bullshit': 467,\n",
       "             'd=that': 468,\n",
       "             'd=he': 469,\n",
       "             'd=constantly': 470,\n",
       "             'd=has': 471,\n",
       "             'd=endure': 472,\n",
       "             'd=planet.': 473,\n",
       "             'ya': 474,\n",
       "             '&gt;': 475,\n",
       "             '400': 476,\n",
       "             'procal': 477,\n",
       "             'other': 478,\n",
       "             'day': 479,\n",
       "             '💁🏼': 480,\n",
       "             'd=chief': 481,\n",
       "             'd=em': 482,\n",
       "             'd=resident': 483,\n",
       "             'd=👩🏼\\u200d⚕️,': 484,\n",
       "             'd=full': 485,\n",
       "             'd=dog': 486,\n",
       "             'd=mom': 487,\n",
       "             'd=🐶,': 488,\n",
       "             'd=closet': 489,\n",
       "             'd=yogi': 490,\n",
       "             'd=🧘🏼\\u200d♀️,': 491,\n",
       "             'd=newbie': 492,\n",
       "             'd=runner': 493,\n",
       "             'd=🏃🏼\\u200d♀️,': 494,\n",
       "             'd=medical': 495,\n",
       "             'd=education': 496,\n",
       "             'd=👩🏼\\u200d🏫': 497,\n",
       "             'ask?': 498,\n",
       "             'course': 499,\n",
       "             \"i'm\": 500,\n",
       "             'ashamed.': 501,\n",
       "             '.THIS_IS_A_MENTION': 502,\n",
       "             ',': 503,\n",
       "             'more': 504,\n",
       "             'ashamed': 505,\n",
       "             'fellow': 506,\n",
       "             'a…': 507,\n",
       "             'd=like': 508,\n",
       "             'd=twitter': 509,\n",
       "             'd=will': 510,\n",
       "             'd=love': 511,\n",
       "             'd=facebook.': 512,\n",
       "             '1999': 513,\n",
       "             'game': 514,\n",
       "             '7': 515,\n",
       "             'pens': 516,\n",
       "             'devils.': 517,\n",
       "             '6': 518,\n",
       "             'memorable': 519,\n",
       "             'me.': 520,\n",
       "             'thought': 521,\n",
       "             'could': 522,\n",
       "             'last': 523,\n",
       "             'ever.': 524,\n",
       "             'd=slightly': 525,\n",
       "             'd=freelance': 526,\n",
       "             'd=writer': 527,\n",
       "             'd=photographer.': 528,\n",
       "             'd=hockey': 529,\n",
       "             'd=fan,': 530,\n",
       "             'd=roadgeek,': 531,\n",
       "             'd=history': 532,\n",
       "             'd=buff,': 533,\n",
       "             'd=etc.': 534,\n",
       "             'd=oh': 535,\n",
       "             'd=curl,': 536,\n",
       "             'd=play': 537,\n",
       "             'd=softball': 538,\n",
       "             'd=kickball': 539,\n",
       "             'd=too.': 540,\n",
       "             'd=finally': 541,\n",
       "             'd=-': 542,\n",
       "             'd=swim.': 543,\n",
       "             'd=bike.': 544,\n",
       "             'd=run.': 545,\n",
       "             'obsessed': 546,\n",
       "             'spring': 547,\n",
       "             'providence': 548,\n",
       "             '🌸': 549,\n",
       "             'd=brown': 550,\n",
       "             \"d='22\": 551,\n",
       "             'officially': 552,\n",
       "             'love': 553,\n",
       "             '✊🏽': 554,\n",
       "             'd=when': 555,\n",
       "             'd=invisible': 556,\n",
       "             'd=do': 557,\n",
       "             'd=impossible.': 558,\n",
       "             'definitely': 559,\n",
       "             'smell!': 560,\n",
       "             'large': 561,\n",
       "             'tub': 562,\n",
       "             'them': 563,\n",
       "             'home.': 564,\n",
       "             'd=husband,': 565,\n",
       "             'd=father,': 566,\n",
       "             'd=teacher,': 567,\n",
       "             'd=google': 568,\n",
       "             'd=certified': 569,\n",
       "             'd=trainer,': 570,\n",
       "             'd=golfer,': 571,\n",
       "             'd=reader,': 572,\n",
       "             'd=mountain': 573,\n",
       "             'd=dew': 574,\n",
       "             'd=addict,': 575,\n",
       "             'd=math': 576,\n",
       "             'd=curriculum': 577,\n",
       "             'd=tech': 578,\n",
       "             'd=integration': 579,\n",
       "             'd=specialist.': 580,\n",
       "             'd=#edtechheroes': 581,\n",
       "             'yer': 582,\n",
       "             'lookin’': 583,\n",
       "             'tonight,': 584,\n",
       "             'bk': 585,\n",
       "             'd=city': 586,\n",
       "             'd=room': 587,\n",
       "             'd=denizen': 588,\n",
       "             'd=(politics/agencies/nycha/foils)': 589,\n",
       "             'd=new': 590,\n",
       "             'd=york': 591,\n",
       "             'd=post.': 592,\n",
       "             'd=tips?': 593,\n",
       "             'd=212-930-8730;': 594,\n",
       "             'd=nhicksTHIS_IS_A_MENTION': 595,\n",
       "             'd=dms': 596,\n",
       "             'd=open;': 597,\n",
       "             'd=ping': 598,\n",
       "             'd=signal/whatsapp.': 599,\n",
       "             'spring,': 600,\n",
       "             'action': 601,\n",
       "             'new': 602,\n",
       "             'name.': 603,\n",
       "             'always': 604,\n",
       "             'forget.': 605,\n",
       "             'almost': 606,\n",
       "             'it.': 607,\n",
       "             'something': 608,\n",
       "             'gene': 609,\n",
       "             'tina': 610,\n",
       "             'ted…': 611,\n",
       "             'd=pale': 612,\n",
       "             'd=dark': 613,\n",
       "             'add': 614,\n",
       "             'one': 615,\n",
       "             'thing': 616,\n",
       "             'tell': 617,\n",
       "             'your': 618,\n",
       "             'child': 619,\n",
       "             'proud': 620,\n",
       "             'mom': 621,\n",
       "             'dad': 622,\n",
       "             'told…': 623,\n",
       "             'd=leftofcenter|politicaljunkie|mlb|nyyankees|nygiants|starwars|startrek|theresistance|democrat|blm|universalhealth|livingwage|usn|antigua&barbuda|nyc|thebx|': 624,\n",
       "             'know': 625,\n",
       "             'headed': 626,\n",
       "             'hell.': 627,\n",
       "             'man': 628,\n",
       "             '72,': 629,\n",
       "             'much': 630,\n",
       "             'longer': 631,\n",
       "             'meeting…': 632,\n",
       "             'dear': 633,\n",
       "             'god': 634,\n",
       "             '.': 635,\n",
       "             ',y': 636,\n",
       "             'heart': 637,\n",
       "             'just': 638,\n",
       "             'stopped': 639,\n",
       "             '😳': 640,\n",
       "             'd=yes,': 641,\n",
       "             \"d=i'm\": 642,\n",
       "             'd=73': 643,\n",
       "             'd=stupid': 644,\n",
       "             'd=or': 645,\n",
       "             'd=dead.': 646,\n",
       "             'd=so': 647,\n",
       "             'd=talk': 648,\n",
       "             'd=while': 649,\n",
       "             \"d=there's\": 650,\n",
       "             'd=still': 651,\n",
       "             'd=.': 652,\n",
       "             'check': 653,\n",
       "             'dms': 654,\n",
       "             'd=alors': 655,\n",
       "             'd=danse': 656,\n",
       "             'd=multifan,': 657,\n",
       "             'd=stans': 658,\n",
       "             'd=bts': 659,\n",
       "             'he': 660,\n",
       "             'doesn’t': 661,\n",
       "             'meaning': 662,\n",
       "             'lot': 663,\n",
       "             'words': 664,\n",
       "             'd=knew': 665,\n",
       "             'd=what': 666,\n",
       "             'd=know': 667,\n",
       "             'd=now': 668,\n",
       "             \"d=i'd\": 669,\n",
       "             'd=be': 670,\n",
       "             'd=whole': 671,\n",
       "             'd=lot': 672,\n",
       "             'd=smarter': 673,\n",
       "             'd=than': 674,\n",
       "             'brought': 675,\n",
       "             'girls': 676,\n",
       "             '10p!': 677,\n",
       "             'also': 678,\n",
       "             'cheers': 679,\n",
       "             'back!': 680,\n",
       "             '👌🏻💯🙌🏻': 681,\n",
       "             'd=2011': 682,\n",
       "             'd=cosmetology': 683,\n",
       "             'd=grad': 684,\n",
       "             'd=•': 685,\n",
       "             'd=rec': 686,\n",
       "             'd=hitachi': 687,\n",
       "             'd=make': 688,\n",
       "             'd=up': 689,\n",
       "             'd=enthusiast': 690,\n",
       "             'd=wifey': 691,\n",
       "             'd=fur': 692,\n",
       "             'd=mama': 693,\n",
       "             'd=god-coffee-music': 694,\n",
       "             'd=batman': 695,\n",
       "             'd=❤️': 696,\n",
       "             'looking': 697,\n",
       "             'quality,': 698,\n",
       "             'christian': 699,\n",
       "             'education': 700,\n",
       "             'son': 701,\n",
       "             'daughter,': 702,\n",
       "             'summit': 703,\n",
       "             'academy.…': 704,\n",
       "             'd=political': 705,\n",
       "             'd=junkie': 706,\n",
       "             'd=face': 707,\n",
       "             'd=penndot': 708,\n",
       "             'd=nepa.': 709,\n",
       "             'little': 710,\n",
       "             'threw': 711,\n",
       "             'up.': 712,\n",
       "             'd=mi': 713,\n",
       "             'd=vicio': 714,\n",
       "             'd=más': 715,\n",
       "             'd=grande': 716,\n",
       "             'd=es': 717,\n",
       "             'd=ser': 718,\n",
       "             'd=alegre.': 719,\n",
       "             'd=#incomparable': 720,\n",
       "             'd=#tigresmanda': 721,\n",
       "             'holy': 722,\n",
       "             'shit,': 723,\n",
       "             'arriola?': 724,\n",
       "             'now': 725,\n",
       "             'even': 726,\n",
       "             'mad.': 727,\n",
       "             \"d=i've\": 728,\n",
       "             'd=been': 729,\n",
       "             'd=called': 730,\n",
       "             \"d='alt-left\": 731,\n",
       "             'd=ira': 732,\n",
       "             \"d=glass.'\": 733,\n",
       "             'nah...': 734,\n",
       "             'think': 735,\n",
       "             \"you're\": 736,\n",
       "             'company.': 737,\n",
       "             'mean,': 738,\n",
       "             'hell...': 739,\n",
       "             'no': 740,\n",
       "             'arena.': 741,\n",
       "             'd=1st': 742,\n",
       "             'd=lady': 743,\n",
       "             'd=sports-talk': 744,\n",
       "             'd=radio': 745,\n",
       "             'd=houston🗽🏆🔥': 746,\n",
       "             'd=cohost': 747,\n",
       "             'd=sean': 748,\n",
       "             'd=unfiltered': 749,\n",
       "             'd=podcast': 750,\n",
       "             'never': 751,\n",
       "             'much.': 752,\n",
       "             'yay': 753,\n",
       "             'socializing.': 754,\n",
       "             '@': 755,\n",
       "             \"veniero's\": 756,\n",
       "             'd=writes': 757,\n",
       "             'd=professional': 758,\n",
       "             'd=wrestling': 759,\n",
       "             'd=charmingly': 760,\n",
       "             'd=snarky': 761,\n",
       "             'd=twist.🙃': 762,\n",
       "             'd=currently': 763,\n",
       "             'd=writing': 764,\n",
       "             'right': 765,\n",
       "             'came': 766,\n",
       "             'idea': 767,\n",
       "             'mind,': 768,\n",
       "             'body': 769,\n",
       "             'soul?': 770,\n",
       "             'd=🌊🌞❤️🎧🙏🏼🗺i’m': 771,\n",
       "             'd=looking': 772,\n",
       "             'd=beautiful': 773,\n",
       "             'd=man': 774,\n",
       "             'd=that’s': 775,\n",
       "             'd=sweet,': 776,\n",
       "             'd=makes': 777,\n",
       "             'd=us': 778,\n",
       "             'd=values': 779,\n",
       "             'd=opinion.': 780,\n",
       "             'd=djsoleraTHIS_IS_A_MENTION': 781,\n",
       "             'cool': 782,\n",
       "             'woman🤷🏻\\u200d♂️': 783,\n",
       "             'd=nothin': 784,\n",
       "             'd=special': 785,\n",
       "             'd=me,': 786,\n",
       "             'd=christian': 787,\n",
       "             'd=conservative': 788,\n",
       "             'd=who': 789,\n",
       "             'd=loves': 790,\n",
       "             'd=god&': 791,\n",
       "             'd=jesus': 792,\n",
       "             'd=christ': 793,\n",
       "             'd=way': 794,\n",
       "             'd=through': 795,\n",
       "             \"d=that's\": 796,\n",
       "             'd=trying': 797,\n",
       "             'd=destroy': 798,\n",
       "             'd=those': 799,\n",
       "             'd=principles': 800,\n",
       "             'speaking': 801,\n",
       "             'facts': 802,\n",
       "             'd=news': 803,\n",
       "             'd=anchor/director': 804,\n",
       "             'd=black,': 805,\n",
       "             'd=educated': 806,\n",
       "             'd=broke': 807,\n",
       "             'd=podcast,': 808,\n",
       "             'd=wsb-am': 809,\n",
       "             'd=traffic': 810,\n",
       "             'd=reporter,': 811,\n",
       "             'd=cox': 812,\n",
       "             'd=media': 813,\n",
       "             'd=group': 814,\n",
       "             'd=atl': 815,\n",
       "             'd=radio,': 816,\n",
       "             'd=rts': 817,\n",
       "             'd=🚫': 818,\n",
       "             'd=endorsements': 819,\n",
       "             'channel': 820,\n",
       "             'drupal': 821,\n",
       "             'slack': 822,\n",
       "             'gitter': 823,\n",
       "             'where': 824,\n",
       "             'can': 825,\n",
       "             'ask': 826,\n",
       "             'questions': 827,\n",
       "             'too.': 828,\n",
       "             'd=drupal,': 829,\n",
       "             'd=improv,': 830,\n",
       "             'd=theatre': 831,\n",
       "             'd=producer,': 832,\n",
       "             'd=podcaster,': 833,\n",
       "             'd=community': 834,\n",
       "             'd=building,': 835,\n",
       "             'd=photowalking,': 836,\n",
       "             'd=coworking,': 837,\n",
       "             'd=beer,': 838,\n",
       "             'd=hugs,': 839,\n",
       "             'd=gardening,': 840,\n",
       "             'd=eating': 841,\n",
       "             'd=local.': 842,\n",
       "             'd=mentor': 843,\n",
       "             'd=startups': 844,\n",
       "             'd=+': 845,\n",
       "             'd=giants.': 846,\n",
       "             'cold?': 847,\n",
       "             'd=ruling': 848,\n",
       "             'd=royal': 849,\n",
       "             'd=👷\\u200d♀️': 850,\n",
       "             'figured': 851,\n",
       "             'total': 852,\n",
       "             'texas': 853,\n",
       "             'trip': 854,\n",
       "             'dz': 855,\n",
       "             'comedy': 856,\n",
       "             'both': 857,\n",
       "             'plano': 858,\n",
       "             'ft': 859,\n",
       "             'worth.': 860,\n",
       "             'its': 861,\n",
       "             'birthday': 862,\n",
       "             'so…': 863,\n",
       "             'd=works': 864,\n",
       "             'd=chipotle': 865,\n",
       "             'd=kroger,': 866,\n",
       "             'd=family': 867,\n",
       "             'd=friends.': 868,\n",
       "             'd=big': 869,\n",
       "             'd=dz': 870,\n",
       "             'd=comedy': 871,\n",
       "             'd=huge': 872,\n",
       "             'd=fan.': 873,\n",
       "             'd=anxiety': 874,\n",
       "             'd=sufferer.': 875,\n",
       "             'hear': 876,\n",
       "             'news': 877,\n",
       "             'today': 878,\n",
       "             'healthy!!': 879,\n",
       "             'd=yourself': 880,\n",
       "             'd=|': 881,\n",
       "             'd=holy': 882,\n",
       "             'd=cross': 883,\n",
       "             'been': 884,\n",
       "             'run': 885,\n",
       "             'gators': 886,\n",
       "             'week.': 887,\n",
       "             'd=capital': 888,\n",
       "             'd=gazette': 889,\n",
       "             'd=—': 890,\n",
       "             'd=schools.': 891,\n",
       "             'd=🎓:': 892,\n",
       "             'd=send': 893,\n",
       "             'd=tips': 894,\n",
       "             'd=cool': 895,\n",
       "             'd=stuff': 896,\n",
       "             'd=athlete': 897,\n",
       "             'd=did': 898,\n",
       "             'd=kfominykhTHIS_IS_A_MENTION': 899,\n",
       "             'want': 900,\n",
       "             'chime': 901,\n",
       "             'say': 902,\n",
       "             'bit': 903,\n",
       "             'bullet': 904,\n",
       "             'subscription': 905,\n",
       "             'afternoon': 906,\n",
       "             'purely': 907,\n",
       "             'because': 908,\n",
       "             'ti…': 909,\n",
       "             'd=screenwriter': 910,\n",
       "             'd=(mostly': 911,\n",
       "             'd=horror).': 912,\n",
       "             'd=junkie.': 913,\n",
       "             'd=part-time': 914,\n",
       "             'd=hobbit.': 915,\n",
       "             'd=work': 916,\n",
       "             'd=tv': 917,\n",
       "             'd=films.': 918,\n",
       "             'd=comment': 919,\n",
       "             'd=horror': 920,\n",
       "             'd=films,': 921,\n",
       "             'd=baseball.': 922,\n",
       "             'd=find': 923,\n",
       "             'throwback': 924,\n",
       "             'd=look': 925,\n",
       "             'd=im': 926,\n",
       "             'd=suave': 927,\n",
       "             'nice': 928,\n",
       "             'job,THIS_IS_A_MENTION': 929,\n",
       "             'easter': 930,\n",
       "             'luck': 931,\n",
       "             'carrying': 932,\n",
       "             'over!': 933,\n",
       "             '💰⚾️': 934,\n",
       "             'looks': 935,\n",
       "             'serial': 936,\n",
       "             'killer': 937,\n",
       "             'hands': 938,\n",
       "             'folks!': 939,\n",
       "             '#seenoevil': 940,\n",
       "             'd=kind!': 941,\n",
       "             'd=considerate!': 942,\n",
       "             'd=loyal!': 943,\n",
       "             'd=wise!': 944,\n",
       "             'd=helpful': 945,\n",
       "             'd=needs': 946,\n",
       "             'd=friend': 947,\n",
       "             'd=to!': 948,\n",
       "             'stop,': 949,\n",
       "             'sound': 950,\n",
       "             'dumb!!!': 951,\n",
       "             'playoff': 952,\n",
       "             'pg': 953,\n",
       "             'use': 954,\n",
       "             'give': 955,\n",
       "             'heats': 956,\n",
       "             'business': 957,\n",
       "             'by': 958,\n",
       "             'himself': 959,\n",
       "             'outta': 960,\n",
       "             'dawg.': 961,\n",
       "             'd=#whataboutyoursoul': 962,\n",
       "             'ain’t': 963,\n",
       "             'paying': 964,\n",
       "             'see': 965,\n",
       "             'interpol': 966,\n",
       "             'd=colorado': 967,\n",
       "             'd=buffaloes,': 968,\n",
       "             'd=rapids': 969,\n",
       "             'd=rockies': 970,\n",
       "             'd=beat.': 971,\n",
       "             'd=words': 972,\n",
       "             'd=buffstampede,': 973,\n",
       "             'd=pxp': 974,\n",
       "             'd=boulder': 975,\n",
       "             'd=kid,': 976,\n",
       "             'd=cu': 977,\n",
       "             'd=grad.': 978,\n",
       "             'venture': 979,\n",
       "             'capital': 980,\n",
       "             'firm': 981,\n",
       "             'set': 982,\n",
       "             'invest': 983,\n",
       "             'billion': 984,\n",
       "             'dollars': 985,\n",
       "             'into': 986,\n",
       "             'cryptocurrencies': 987,\n",
       "             'blockchain.': 988,\n",
       "             'via': 989,\n",
       "             'd=adj.': 990,\n",
       "             'd=prof': 991,\n",
       "             'd=internet': 992,\n",
       "             'd=studies': 993,\n",
       "             'd=strategy': 994,\n",
       "             'd=house/fcc/ibm/THIS_IS_A_MENTION': 995,\n",
       "             'd=accelerating': 996,\n",
       "             'd=net': 997,\n",
       "             'd=cloud': 998,\n",
       "             'including': 999,\n",
       "             ...})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term counts for tweet 1.\n",
    "X[1].data  # \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# What word does each term index correspond to?\n",
    "# Convert term->index dict into index->term dict\n",
    "index2term = {i: t for t, i in vocabulary.items()}\n",
    "print(index2term[2])\n",
    "print(X[0, 2])\n",
    "# So, the term \"the\" (index 15) appears in user 200's tweet one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet 1 starts at col_ind= 20\n",
      "tweet 2 starts at col_ind= 48\n",
      "so, the columns that are non-zero for tweet 1 are:\n",
      "[13 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\n",
      " 43 44 45 46]\n",
      "and the data associated with those cells are:\n",
      "[1. 1. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('tweet 1 starts at col_ind=', X.indptr[1])\n",
    "print('tweet 2 starts at col_ind=', X.indptr[2])\n",
    "print('so, the columns that are non-zero for tweet 1 are:')\n",
    "print(X.indices[X.indptr[1]:X.indptr[2]])\n",
    "print('and the data associated with those cells are:')\n",
    "print(X.data[X.indptr[1]:X.indptr[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet 0:\n",
      "   (0, 0)\t1.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 7)\t1.0\n",
      "  (0, 8)\t1.0\n",
      "  (0, 9)\t1.0\n",
      "  (0, 10)\t1.0\n",
      "  (0, 11)\t1.0\n",
      "  (0, 12)\t1.0\n",
      "  (0, 13)\t1.0\n",
      "  (0, 14)\t1.0\n",
      "  (0, 15)\t1.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 17)\t1.0\n",
      "  (0, 18)\t1.0\n",
      "  (0, 19)\t1.0 \n",
      "\n",
      "tweet 1:\n",
      "   (0, 13)\t1.0\n",
      "  (0, 20)\t1.0\n",
      "  (0, 21)\t3.0\n",
      "  (0, 22)\t1.0\n",
      "  (0, 23)\t1.0\n",
      "  (0, 24)\t1.0\n",
      "  (0, 25)\t1.0\n",
      "  (0, 26)\t1.0\n",
      "  (0, 27)\t1.0\n",
      "  (0, 28)\t1.0\n",
      "  (0, 29)\t1.0\n",
      "  (0, 30)\t1.0\n",
      "  (0, 31)\t1.0\n",
      "  (0, 32)\t1.0\n",
      "  (0, 33)\t1.0\n",
      "  (0, 34)\t1.0\n",
      "  (0, 35)\t1.0\n",
      "  (0, 36)\t1.0\n",
      "  (0, 37)\t1.0\n",
      "  (0, 38)\t1.0\n",
      "  (0, 39)\t1.0\n",
      "  (0, 40)\t1.0\n",
      "  (0, 41)\t1.0\n",
      "  (0, 42)\t1.0\n",
      "  (0, 43)\t1.0\n",
      "  (0, 44)\t1.0\n",
      "  (0, 45)\t1.0\n",
      "  (0, 46)\t1.0 \n",
      "\n",
      "tweet 2:\n",
      "   (0, 20)\t1.0\n",
      "  (0, 37)\t1.0\n",
      "  (0, 47)\t1.0\n",
      "  (0, 48)\t1.0\n",
      "  (0, 49)\t1.0\n",
      "  (0, 50)\t1.0\n",
      "  (0, 51)\t1.0\n",
      "  (0, 52)\t1.0\n",
      "  (0, 53)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print('tweet 0:\\n', X[0], '\\n')\n",
    "print('tweet 1:\\n', X[1], '\\n')\n",
    "print('tweet 2:\\n', X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X * beta for tweet 1= 30.0\n",
      "which is the same as the sum 30.0, since beta=[1...1]\n"
     ]
    }
   ],
   "source": [
    "# Compute z = X * \\beta, where X is a CSR matrix.\n",
    "import numpy as np\n",
    "beta = np.ones(len(vocabulary))  # assume Beta = vector of 1s\n",
    "z = np.zeros(len(tweets))\n",
    "for i in range(len(tweets)):  # for each row.\n",
    "    for j in range(X.indptr[i], X.indptr[i+1]): # for each col.\n",
    "        colidx = X.indices[j]\n",
    "        z[i] += beta[colidx] * X.data[j]\n",
    "print('X * beta for tweet 1=', z[1])\n",
    "print('which is the same as the sum %.1f, since beta=[1...1]' %\n",
    "      X[1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create a list of gender labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender labels: Counter({0: 2963, 1: 2037})\n"
     ]
    }
   ],
   "source": [
    "# y is a 1d numpy array of gender labels.\n",
    "# Let 1=Female, 0=Male.\n",
    "import numpy as np\n",
    "\n",
    "def get_gender(tweet, male_names, female_names):\n",
    "    name = get_first_name(tweet)\n",
    "    if name in female_names:\n",
    "        return 1\n",
    "    elif name in male_names:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "y = np.array([get_gender(t, male_names, female_names) for t in tweets])\n",
    "print('gender labels:', Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Fit a Logistic Regression classifier to predict gender from profile/tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 5-fold cross-validation\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def do_cross_val(X, y, nfolds):\n",
    "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
    "    cv = KFold(n_splits=nfolds, random_state=42, shuffle=True)\n",
    "    accuracies = []\n",
    "    for train_idx, test_idx in cv.split(X):\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        predicted = clf.predict(X[test_idx])\n",
    "        acc = accuracy_score(y[test_idx], predicted)\n",
    "        accuracies.append(acc)\n",
    "    avg = np.mean(accuracies)\n",
    "    print(np.std(accuracies))\n",
    "    print(accuracies)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agericke/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011822013364905331\n",
      "[0.719, 0.73, 0.753, 0.745, 0.734]\n",
      "avg accuracy 0.7362\n"
     ]
    }
   ],
   "source": [
    "print('avg accuracy', do_cross_val(X, y, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does tokenization affect accuracy?\n",
    "# Collapse urls and mentions; ignore description prefix.\n",
    "def run_all(tweets, use_descr=True, lowercase=True,\n",
    "            keep_punctuation=True, descr_prefix=None,\n",
    "            collapse_urls=True, collapse_mentions=True):\n",
    "    \n",
    "    tokens_list = [tweet2tokens(t, use_descr, lowercase,\n",
    "                            keep_punctuation, descr_prefix,\n",
    "                            collapse_urls, collapse_mentions)\n",
    "                  for t in tweets]\n",
    "    vocabulary = make_vocabulary(tokens_list)\n",
    "    X = make_feature_matrix(tokens_list, vocabulary)\n",
    "    acc = do_cross_val(X, y, 5)\n",
    "    print('acc=', acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
      "30498 unique terms in vocabulary\n",
      "0.011822013364905331\n",
      "[0.719, 0.73, 0.753, 0.745, 0.734]\n",
      "acc= 0.7362\n",
      "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
      "35285 unique terms in vocabulary\n",
      "0.01149956520917205\n",
      "[0.725, 0.742, 0.757, 0.748, 0.731]\n",
      "acc= 0.7406\n",
      "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
      "32522 unique terms in vocabulary\n",
      "0.011391224692718523\n",
      "[0.721, 0.732, 0.754, 0.746, 0.736]\n",
      "acc= 0.7378\n",
      "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
      "37309 unique terms in vocabulary\n",
      "0.011855800268223155\n",
      "[0.722, 0.744, 0.754, 0.751, 0.733]\n",
      "acc= 0.7407999999999999\n",
      "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\n",
      "26767 unique terms in vocabulary\n",
      "0.012611106216347569\n",
      "[0.721, 0.721, 0.755, 0.729, 0.736]\n",
      "acc= 0.7323999999999999\n",
      "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\n",
      "31493 unique terms in vocabulary\n",
      "0.012354756169184411\n",
      "[0.718, 0.724, 0.754, 0.728, 0.734]\n",
      "acc= 0.7315999999999999\n",
      "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\n",
      "28792 unique terms in vocabulary\n",
      "0.01080000000000001\n",
      "[0.719, 0.727, 0.751, 0.727, 0.734]\n",
      "acc= 0.7316\n",
      "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\n",
      "33518 unique terms in vocabulary\n",
      "0.012674383614203899\n",
      "[0.722, 0.724, 0.757, 0.728, 0.732]\n",
      "acc= 0.7326\n",
      "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
      "20426 unique terms in vocabulary\n",
      "0.014119490075778245\n",
      "[0.729, 0.749, 0.767, 0.757, 0.734]\n",
      "acc= 0.7472000000000001\n",
      "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
      "25029 unique terms in vocabulary\n",
      "0.016054905792311596\n",
      "[0.726, 0.754, 0.768, 0.76, 0.733]\n",
      "acc= 0.7482\n",
      "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
      "22499 unique terms in vocabulary\n",
      "0.01407693148381423\n",
      "[0.73, 0.75, 0.765, 0.759, 0.732]\n",
      "acc= 0.7472\n",
      "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
      "27098 unique terms in vocabulary\n",
      "0.015497096502248427\n",
      "[0.727, 0.752, 0.77, 0.756, 0.734]\n",
      "acc= 0.7478\n",
      "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\n",
      "16644 unique terms in vocabulary\n",
      "0.008867919710958154\n",
      "[0.724, 0.743, 0.747, 0.748, 0.736]\n",
      "acc= 0.7395999999999999\n",
      "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\n",
      "21134 unique terms in vocabulary\n",
      "0.010892199043352091\n",
      "[0.726, 0.751, 0.753, 0.746, 0.731]\n",
      "acc= 0.7414\n",
      "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\n",
      "18707 unique terms in vocabulary\n",
      "0.008611620056644401\n",
      "[0.726, 0.744, 0.747, 0.746, 0.731]\n",
      "acc= 0.7388\n",
      "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\n",
      "23187 unique terms in vocabulary\n",
      "0.010609429767899884\n",
      "[0.726, 0.751, 0.754, 0.748, 0.735]\n",
      "acc= 0.7428\n",
      "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
      "34827 unique terms in vocabulary\n",
      "0.012696456198483114\n",
      "[0.716, 0.733, 0.753, 0.739, 0.724]\n",
      "acc= 0.733\n",
      "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
      "39632 unique terms in vocabulary\n",
      "0.012749901960407393\n",
      "[0.712, 0.732, 0.752, 0.735, 0.73]\n",
      "acc= 0.7322\n",
      "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
      "36851 unique terms in vocabulary\n",
      "0.010759182125050221\n",
      "[0.712, 0.727, 0.744, 0.737, 0.731]\n",
      "acc= 0.7302\n",
      "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
      "41656 unique terms in vocabulary\n",
      "0.011088733020503299\n",
      "[0.711, 0.734, 0.745, 0.733, 0.728]\n",
      "acc= 0.7302\n",
      "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\n",
      "30925 unique terms in vocabulary\n",
      "0.014710540438746648\n",
      "[0.714, 0.715, 0.754, 0.734, 0.733]\n",
      "acc= 0.73\n",
      "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\n",
      "35680 unique terms in vocabulary\n",
      "0.014352700094407337\n",
      "[0.713, 0.72, 0.754, 0.736, 0.737]\n",
      "acc= 0.732\n",
      "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\n",
      "32950 unique terms in vocabulary\n",
      "0.014408330923462316\n",
      "[0.713, 0.716, 0.751, 0.736, 0.739]\n",
      "acc= 0.7309999999999999\n",
      "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\n",
      "37705 unique terms in vocabulary\n",
      "0.015104966070799378\n",
      "[0.712, 0.72, 0.755, 0.74, 0.734]\n",
      "acc= 0.7322\n",
      "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
      "25012 unique terms in vocabulary\n",
      "0.013044539087296273\n",
      "[0.738, 0.747, 0.769, 0.745, 0.73]\n",
      "acc= 0.7458\n",
      "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
      "29680 unique terms in vocabulary\n",
      "0.010457533169921106\n",
      "[0.733, 0.748, 0.76, 0.75, 0.733]\n",
      "acc= 0.7447999999999999\n",
      "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
      "27120 unique terms in vocabulary\n",
      "0.01230284519938377\n",
      "[0.734, 0.744, 0.762, 0.748, 0.726]\n",
      "acc= 0.7428000000000001\n",
      "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
      "31787 unique terms in vocabulary\n",
      "0.011558546621439921\n",
      "[0.733, 0.752, 0.76, 0.75, 0.73]\n",
      "acc= 0.745\n",
      "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\n",
      "20913 unique terms in vocabulary\n",
      "0.004690415759823434\n",
      "[0.727, 0.738, 0.74, 0.736, 0.739]\n",
      "acc= 0.736\n",
      "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\n",
      "25487 unique terms in vocabulary\n",
      "0.009516301802696265\n",
      "[0.723, 0.747, 0.749, 0.737, 0.745]\n",
      "acc= 0.7402\n",
      "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\n",
      "23003 unique terms in vocabulary\n",
      "0.006046486583132395\n",
      "[0.725, 0.739, 0.74, 0.735, 0.742]\n",
      "acc= 0.7362\n",
      "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\n",
      "27573 unique terms in vocabulary\n",
      "0.006529931086925809\n",
      "[0.729, 0.744, 0.745, 0.738, 0.747]\n",
      "acc= 0.7405999999999999\n",
      "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
      "14067 unique terms in vocabulary\n",
      "0.007277362159464108\n",
      "[0.614, 0.626, 0.635, 0.624, 0.632]\n",
      "acc= 0.6262000000000001\n",
      "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
      "17661 unique terms in vocabulary\n",
      "0.00667532770731146\n",
      "[0.623, 0.639, 0.64, 0.63, 0.639]\n",
      "acc= 0.6342000000000001\n",
      "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
      "15883 unique terms in vocabulary\n",
      "0.009907572861200677\n",
      "[0.616, 0.624, 0.637, 0.613, 0.636]\n",
      "acc= 0.6252000000000001\n",
      "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
      "19477 unique terms in vocabulary\n",
      "0.008485281374238578\n",
      "[0.622, 0.64, 0.644, 0.626, 0.638]\n",
      "acc= 0.634\n",
      "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\n",
      "14067 unique terms in vocabulary\n",
      "0.007277362159464108\n",
      "[0.614, 0.626, 0.635, 0.624, 0.632]\n",
      "acc= 0.6262000000000001\n",
      "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\n",
      "17661 unique terms in vocabulary\n",
      "0.00667532770731146\n",
      "[0.623, 0.639, 0.64, 0.63, 0.639]\n",
      "acc= 0.6342000000000001\n",
      "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\n",
      "15883 unique terms in vocabulary\n",
      "0.009907572861200677\n",
      "[0.616, 0.624, 0.637, 0.613, 0.636]\n",
      "acc= 0.6252000000000001\n",
      "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\n",
      "19477 unique terms in vocabulary\n",
      "0.008485281374238578\n",
      "[0.622, 0.64, 0.644, 0.626, 0.638]\n",
      "acc= 0.634\n",
      "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
      "9506 unique terms in vocabulary\n",
      "0.012576167937810002\n",
      "[0.603, 0.618, 0.639, 0.611, 0.608]\n",
      "acc= 0.6158\n",
      "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
      "12977 unique terms in vocabulary\n",
      "0.011124747188138712\n",
      "[0.609, 0.628, 0.639, 0.624, 0.611]\n",
      "acc= 0.6222\n",
      "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
      "11322 unique terms in vocabulary\n",
      "0.01235151812531562\n",
      "[0.608, 0.614, 0.639, 0.611, 0.604]\n",
      "acc= 0.6152\n",
      "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
      "14793 unique terms in vocabulary\n",
      "0.012815615474880646\n",
      "[0.61, 0.623, 0.645, 0.624, 0.61]\n",
      "acc= 0.6224000000000001\n",
      "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\n",
      "9506 unique terms in vocabulary\n",
      "0.012576167937810002\n",
      "[0.603, 0.618, 0.639, 0.611, 0.608]\n",
      "acc= 0.6158\n",
      "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\n",
      "12977 unique terms in vocabulary\n",
      "0.011124747188138712\n",
      "[0.609, 0.628, 0.639, 0.624, 0.611]\n",
      "acc= 0.6222\n",
      "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\n",
      "11322 unique terms in vocabulary\n",
      "0.01235151812531562\n",
      "[0.608, 0.614, 0.639, 0.611, 0.604]\n",
      "acc= 0.6152\n",
      "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14793 unique terms in vocabulary\n",
      "0.012815615474880646\n",
      "[0.61, 0.623, 0.645, 0.624, 0.61]\n",
      "acc= 0.6224000000000001\n",
      "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
      "15976 unique terms in vocabulary\n",
      "0.00917387595294378\n",
      "[0.617, 0.615, 0.639, 0.633, 0.625]\n",
      "acc= 0.6258\n",
      "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
      "19579 unique terms in vocabulary\n",
      "0.0037094473981982845\n",
      "[0.628, 0.628, 0.638, 0.632, 0.63]\n",
      "acc= 0.6312\n",
      "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
      "17792 unique terms in vocabulary\n",
      "0.009579144011862446\n",
      "[0.613, 0.617, 0.636, 0.636, 0.629]\n",
      "acc= 0.6262000000000001\n",
      "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
      "21395 unique terms in vocabulary\n",
      "0.007110555533852477\n",
      "[0.623, 0.623, 0.641, 0.635, 0.634]\n",
      "acc= 0.6312\n",
      "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\n",
      "15976 unique terms in vocabulary\n",
      "0.00917387595294378\n",
      "[0.617, 0.615, 0.639, 0.633, 0.625]\n",
      "acc= 0.6258\n",
      "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\n",
      "19579 unique terms in vocabulary\n",
      "0.0037094473981982845\n",
      "[0.628, 0.628, 0.638, 0.632, 0.63]\n",
      "acc= 0.6312\n",
      "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\n",
      "17792 unique terms in vocabulary\n",
      "0.009579144011862446\n",
      "[0.613, 0.617, 0.636, 0.636, 0.629]\n",
      "acc= 0.6262000000000001\n",
      "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\n",
      "21395 unique terms in vocabulary\n",
      "0.007110555533852477\n",
      "[0.623, 0.623, 0.641, 0.635, 0.634]\n",
      "acc= 0.6312\n",
      "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
      "11563 unique terms in vocabulary\n",
      "0.012939860895697462\n",
      "[0.607, 0.61, 0.639, 0.623, 0.604]\n",
      "acc= 0.6166\n",
      "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
      "15063 unique terms in vocabulary\n",
      "0.014527215837867915\n",
      "[0.616, 0.625, 0.653, 0.639, 0.615]\n",
      "acc= 0.6296000000000002\n",
      "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
      "13380 unique terms in vocabulary\n",
      "0.015594870951694356\n",
      "[0.608, 0.61, 0.642, 0.627, 0.598]\n",
      "acc= 0.617\n",
      "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
      "16880 unique terms in vocabulary\n",
      "0.016904437287292365\n",
      "[0.614, 0.62, 0.656, 0.636, 0.61]\n",
      "acc= 0.6272\n",
      "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\n",
      "11563 unique terms in vocabulary\n",
      "0.012939860895697462\n",
      "[0.607, 0.61, 0.639, 0.623, 0.604]\n",
      "acc= 0.6166\n",
      "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\n",
      "15063 unique terms in vocabulary\n",
      "0.014527215837867915\n",
      "[0.616, 0.625, 0.653, 0.639, 0.615]\n",
      "acc= 0.6296000000000002\n",
      "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\n",
      "13380 unique terms in vocabulary\n",
      "0.015594870951694356\n",
      "[0.608, 0.61, 0.642, 0.627, 0.598]\n",
      "acc= 0.617\n",
      "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\n",
      "16880 unique terms in vocabulary\n",
      "0.016904437287292365\n",
      "[0.614, 0.62, 0.656, 0.636, 0.61]\n",
      "acc= 0.6272\n"
     ]
    }
   ],
   "source": [
    "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention']\n",
    "option_iter = product(use_descr_opts, lowercase_opts,\n",
    "                       keep_punctuation_opts,\n",
    "                       descr_prefix_opts, url_opts,\n",
    "                       mention_opts)\n",
    "results = []\n",
    "for options in option_iter:\n",
    "    option_str = '\\t'.join('%s=%s' % (name, opt) for name, opt\n",
    "                           in zip(argnames, options))\n",
    "    print(option_str)\n",
    "    acc = run_all(tweets, *options)\n",
    "    results.append((acc, options))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7482 use_descr=True  lower=True  punct=False  prefix=d=  url=True  mention=False\n",
      "0.7478 use_descr=True  lower=True  punct=False  prefix=d=  url=False  mention=False\n",
      "0.7472 use_descr=True  lower=True  punct=False  prefix=d=  url=True  mention=True\n",
      "0.7472 use_descr=True  lower=True  punct=False  prefix=d=  url=False  mention=True\n",
      "0.7458 use_descr=True  lower=False  punct=False  prefix=d=  url=True  mention=True\n",
      "0.7450 use_descr=True  lower=False  punct=False  prefix=d=  url=False  mention=False\n",
      "0.7448 use_descr=True  lower=False  punct=False  prefix=d=  url=True  mention=False\n",
      "0.7428 use_descr=True  lower=False  punct=False  prefix=d=  url=False  mention=True\n",
      "0.7428 use_descr=True  lower=True  punct=False  prefix=  url=False  mention=False\n",
      "0.7414 use_descr=True  lower=True  punct=False  prefix=  url=True  mention=False\n",
      "0.7408 use_descr=True  lower=True  punct=True  prefix=d=  url=False  mention=False\n",
      "0.7406 use_descr=True  lower=True  punct=True  prefix=d=  url=True  mention=False\n",
      "0.7406 use_descr=True  lower=False  punct=False  prefix=  url=False  mention=False\n",
      "0.7402 use_descr=True  lower=False  punct=False  prefix=  url=True  mention=False\n",
      "0.7396 use_descr=True  lower=True  punct=False  prefix=  url=True  mention=True\n",
      "0.7388 use_descr=True  lower=True  punct=False  prefix=  url=False  mention=True\n",
      "0.7378 use_descr=True  lower=True  punct=True  prefix=d=  url=False  mention=True\n",
      "0.7362 use_descr=True  lower=True  punct=True  prefix=d=  url=True  mention=True\n",
      "0.7362 use_descr=True  lower=False  punct=False  prefix=  url=False  mention=True\n",
      "0.7360 use_descr=True  lower=False  punct=False  prefix=  url=True  mention=True\n",
      "0.7330 use_descr=True  lower=False  punct=True  prefix=d=  url=True  mention=True\n",
      "0.7326 use_descr=True  lower=True  punct=True  prefix=  url=False  mention=False\n",
      "0.7324 use_descr=True  lower=True  punct=True  prefix=  url=True  mention=True\n",
      "0.7322 use_descr=True  lower=False  punct=True  prefix=d=  url=True  mention=False\n",
      "0.7322 use_descr=True  lower=False  punct=True  prefix=  url=False  mention=False\n",
      "0.7320 use_descr=True  lower=False  punct=True  prefix=  url=True  mention=False\n",
      "0.7316 use_descr=True  lower=True  punct=True  prefix=  url=False  mention=True\n",
      "0.7316 use_descr=True  lower=True  punct=True  prefix=  url=True  mention=False\n",
      "0.7310 use_descr=True  lower=False  punct=True  prefix=  url=False  mention=True\n",
      "0.7302 use_descr=True  lower=False  punct=True  prefix=d=  url=False  mention=True\n",
      "0.7302 use_descr=True  lower=False  punct=True  prefix=d=  url=False  mention=False\n",
      "0.7300 use_descr=True  lower=False  punct=True  prefix=  url=True  mention=True\n",
      "0.6342 use_descr=False  lower=True  punct=True  prefix=d=  url=True  mention=False\n",
      "0.6342 use_descr=False  lower=True  punct=True  prefix=  url=True  mention=False\n",
      "0.6340 use_descr=False  lower=True  punct=True  prefix=d=  url=False  mention=False\n",
      "0.6340 use_descr=False  lower=True  punct=True  prefix=  url=False  mention=False\n",
      "0.6312 use_descr=False  lower=False  punct=True  prefix=d=  url=True  mention=False\n",
      "0.6312 use_descr=False  lower=False  punct=True  prefix=d=  url=False  mention=False\n",
      "0.6312 use_descr=False  lower=False  punct=True  prefix=  url=True  mention=False\n",
      "0.6312 use_descr=False  lower=False  punct=True  prefix=  url=False  mention=False\n",
      "0.6296 use_descr=False  lower=False  punct=False  prefix=d=  url=True  mention=False\n",
      "0.6296 use_descr=False  lower=False  punct=False  prefix=  url=True  mention=False\n",
      "0.6272 use_descr=False  lower=False  punct=False  prefix=d=  url=False  mention=False\n",
      "0.6272 use_descr=False  lower=False  punct=False  prefix=  url=False  mention=False\n",
      "0.6262 use_descr=False  lower=True  punct=True  prefix=d=  url=True  mention=True\n",
      "0.6262 use_descr=False  lower=True  punct=True  prefix=  url=True  mention=True\n",
      "0.6262 use_descr=False  lower=False  punct=True  prefix=d=  url=False  mention=True\n",
      "0.6262 use_descr=False  lower=False  punct=True  prefix=  url=False  mention=True\n",
      "0.6258 use_descr=False  lower=False  punct=True  prefix=d=  url=True  mention=True\n",
      "0.6258 use_descr=False  lower=False  punct=True  prefix=  url=True  mention=True\n",
      "0.6252 use_descr=False  lower=True  punct=True  prefix=d=  url=False  mention=True\n",
      "0.6252 use_descr=False  lower=True  punct=True  prefix=  url=False  mention=True\n",
      "0.6224 use_descr=False  lower=True  punct=False  prefix=d=  url=False  mention=False\n",
      "0.6224 use_descr=False  lower=True  punct=False  prefix=  url=False  mention=False\n",
      "0.6222 use_descr=False  lower=True  punct=False  prefix=d=  url=True  mention=False\n",
      "0.6222 use_descr=False  lower=True  punct=False  prefix=  url=True  mention=False\n",
      "0.6170 use_descr=False  lower=False  punct=False  prefix=d=  url=False  mention=True\n",
      "0.6170 use_descr=False  lower=False  punct=False  prefix=  url=False  mention=True\n",
      "0.6166 use_descr=False  lower=False  punct=False  prefix=d=  url=True  mention=True\n",
      "0.6166 use_descr=False  lower=False  punct=False  prefix=  url=True  mention=True\n",
      "0.6158 use_descr=False  lower=True  punct=False  prefix=d=  url=True  mention=True\n",
      "0.6158 use_descr=False  lower=True  punct=False  prefix=  url=True  mention=True\n",
      "0.6152 use_descr=False  lower=True  punct=False  prefix=d=  url=False  mention=True\n",
      "0.6152 use_descr=False  lower=True  punct=False  prefix=  url=False  mention=True\n"
     ]
    }
   ],
   "source": [
    "for r in sorted(results, reverse=True):\n",
    "    print('%.4f' % r[0], '  '.join('%s=%s' % (name, opt) for name, opt in zip(argnames, r[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = dict((v,k) for k,v in vocabulary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top weighted terms for female class:\n",
      "('d=mom', 1.7476924957812148)\n",
      "('d=mom,', 1.5884129128188123)\n",
      "('d=she/her', 1.5181028804416337)\n",
      "('d=alumna', 1.4089658690648104)\n",
      "('d=mother,', 1.2720610498952378)\n",
      "('d=✨', 1.2122067776386518)\n",
      "('d=our', 1.1858025175306444)\n",
      "('d=mom.', 1.1252789076472065)\n",
      "('app', 1.1037215889944685)\n",
      "('d=mother', 1.0590583603889652)\n",
      "('d=loving', 1.0241080750841511)\n",
      "('d=she/her.', 1.017781124891941)\n",
      "('today!', 1.009349802768622)\n",
      "('told', 1.0050504694343796)\n",
      "('d=woman', 0.9628902712622512)\n",
      "('d=:', 0.9560020033356396)\n",
      "('d=girl', 0.949903951512294)\n",
      "('d=trump', 0.9178808364570152)\n",
      "('job', 0.9171617350606579)\n",
      "('—', 0.9153969401724443)\n",
      "\n",
      "top weighted terms for male class:\n",
      "('d=father', -1.8877512892583566)\n",
      "('d=husband,', -1.3502937912948538)\n",
      "('d=dad', -1.3249415655807995)\n",
      "('d=husband', -1.26194956817943)\n",
      "('d=l', -1.1340953170408035)\n",
      "('d=sports', -1.0956303657023583)\n",
      "('birthday', -1.0859370404484419)\n",
      "('d=man', -1.0840524463674461)\n",
      "('d=former', -1.0689713118452118)\n",
      "('d=guy', -1.0446309081684688)\n",
      "('bitcoin', -1.0328686165847907)\n",
      "('d=apple', -0.9961268495445632)\n",
      "('d=father,', -0.9960816244566184)\n",
      "('goes', -0.960506635095793)\n",
      "('d=host', -0.953283111705905)\n",
      "('th…', -0.9453138406252668)\n",
      "('d=➡️', -0.9372710551556314)\n",
      "('d=999.', -0.894549850807486)\n",
      "(\"d=it's\", -0.8883331807433887)\n",
      "('d=things.', -0.8853533102994533)\n"
     ]
    }
   ],
   "source": [
    "# Fit model on all data and print top coef.\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "# Get the learned coefficients for the Positive class.\n",
    "coef = model.coef_[0]\n",
    "# Sort them in descending order.\n",
    "top_coef_ind = np.argsort(coef)[::-1][:20]\n",
    "# Get the names of those features.\n",
    "top_coef_terms = [idx2word[i] for i in top_coef_ind]\n",
    "# Get the weights of those features\n",
    "top_coef = coef[top_coef_ind]\n",
    "# Print the top 10.\n",
    "print('top weighted terms for female class:')\n",
    "print('\\n'.join(str(x) for x in zip(top_coef_terms, top_coef)))\n",
    "\n",
    "# repeat for males\n",
    "top_coef_ind = np.argsort(coef)[:20]\n",
    "top_coef_terms = [idx2word[i] for i in top_coef_ind]\n",
    "top_coef = coef[top_coef_ind]\n",
    "print('\\ntop weighted terms for male class:')\n",
    "print('\\n'.join(str(x) for x in zip(top_coef_terms, top_coef)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
